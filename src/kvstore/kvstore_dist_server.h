/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

/*!
 * Copyright (c) 2015 by Contributors
 * Modifications Copyright (c) 2021 by Contributors at INET-RC
 * \file mxnet_node.h
 * \brief implement mxnet nodes
 */
#ifndef MXNET_KVSTORE_KVSTORE_DIST_SERVER_H_
#define MXNET_KVSTORE_KVSTORE_DIST_SERVER_H_
#include <mxnet/c_api.h>
#include <mxnet/kvstore.h>
#include <ps/ps.h>
#include <queue>
#include <string>
#include <mutex>
#include <condition_variable>
#include <memory>
#include <functional>
#include <future>
#include <vector>
#include <iostream>
#include "./comm.h"
#include "../profiler/profiler.h"
#include "../operator/tensor/elemwise_binary_op-inl.h"
#include "../operator/tensor/init_op.h"

namespace mxnet {
namespace kvstore {

// maintain same order in frontend.
enum class CommandType {
  kController, kSetMultiPrecision, kStopServer, kSyncMode, kSyncGlobalMode,
  kSetGradientCompression, kSetProfilerParams
};

enum class RequestType {
  kDefaultPushPull, kRowSparsePushPull, kCompressedPushPull, kBSCompressedPushPull
};

struct DataHandleType {
  RequestType requestType;
  int dtype;
};

struct PSKV {
  ps::SArray<ps::Key> keys;  // n keys
  ps::SArray<int> lens;  // the length of the i-th value
  int size;
};

struct ComprPSKV {
  PSKV push;
  PSKV pull;
};

/*!
 * Uses Cantor pairing function to generate a unique number given two numbers.
 * This number can also be inverted to find the unique pair whose Cantor value is this number.
 * Ref: https://en.wikipedia.org/wiki/Pairing_function#Cantor_pairing_function
 * \param requestType RequestType
 * \param dtype integer
 * \return Cantor value of arguments
 */
static int GetCommandType(RequestType requestType, int d) {
  int m = static_cast<int>(requestType);
  return (((m + d) * (m + d + 1)) / 2) + d;
}

/*!
 * Unpairs Cantor value and finds the two integers used to pair.
 * Then returns DataHandleType object with those numbers.
 * \param cmd DataHandleCommand generated by GetCommandType function
 * \return DataHandleType
 */
static DataHandleType DepairDataHandleType(int cmd) {
  int w = std::floor((std::sqrt(8 * cmd + 1) - 1)/2);
  int t = ((w * w) + w) / 2;
  int y = cmd - t;
  int x = w - y;
  CHECK_GE(x, 0);
  CHECK_GE(y, 0);
  DataHandleType type;
  type.requestType = static_cast<RequestType>(x);
  type.dtype = y;
  return type;
}

/**
 * \brief executor runs a function using the thread called \ref Start
 */
class Executor {
 public:
  /**
   * \brief start the executor
   */
  void Start() {
    std::unique_lock<std::mutex> lk(mu_);
    while (true) {
      cond_.wait(lk, [this]{return !queue_.empty();});
      Block blk = std::move(queue_.front());
      queue_.pop();
      lk.unlock();

      if (blk.f) {
        blk.f();
        blk.p->set_value();
      } else {
        blk.p->set_value();
        break;
      }
      lk.lock();
    }
  }

  /**
   * \brief function
   */
  typedef std::function<void()> Func;

  /**
   * \brief let the thread called \ref Start to exec a function. threadsafe
   */
  void Exec(const Func& func) {
    Block blk(func);
    auto fut = blk.p->get_future();
    {
      std::lock_guard<std::mutex> lk(mu_);
      queue_.push(std::move(blk));
      cond_.notify_one();
    }
    fut.wait();
  }

  /**
   * \brief stop the thread, threadsafe
   */
  void Stop() {
    Exec(Func());
  }

 private:
  struct Block {
  explicit Block(const Func& func) : f(func), p(std::make_shared<std::promise<void>>()) { }
    Func f;
    std::shared_ptr<std::promise<void>> p;
  };
  std::queue<Block> queue_;
  std::mutex mu_;
  std::condition_variable cond_;
};

class KVStoreDistServer {
 public:
  KVStoreDistServer() {
    using namespace std::placeholders;
    ps_server_ = new ps::KVServer<char>(0);
    static_cast<ps::SimpleApp*>(ps_server_)->set_request_handle(
      std::bind(&KVStoreDistServer::CommandHandle, this, _1, _2));
    ps_server_->set_request_handle(
      std::bind(&KVStoreDistServer::DataHandleEx, this, _1, _2, _3));
    ps_server_->set_request_global_handle(
      std::bind(&KVStoreDistServer::WorkersMerge, this, _1, _2, _3));
    gradient_compression_ = std::make_shared<GradientCompression>();
    bigarray_bound_ = dmlc::GetEnv("MXNET_KVSTORE_BIGARRAY_BOUND", 1000 * 1000);
    size_lower_bound = dmlc::GetEnv("MXNET_KVSTORE_SIZE_LOWER_BOUND", 200 * 1000);
    log_verbose_ = dmlc::GetEnv("MXNET_KVSTORE_DIST_ROW_SPARSE_VERBOSE", false);
    use_hfa = dmlc::GetEnv("MXNET_KVSTORE_USE_HFA", false);
    period_k1 = dmlc::GetEnv("MXNET_KVSTORE_HFA_K1", 1);
    period_k2 = dmlc::GetEnv("MXNET_KVSTORE_HFA_K2", 1);
    local_iters = 0;
    // explicitly set to false, avoid wrong dtype of store_ when net is float16
    multi_precision_ = false;
  }

  ~KVStoreDistServer() {
    profiler::Profiler::Get()->SetState(profiler::Profiler::ProfilerState(0));
    delete ps_server_;
  }

  void set_controller(const KVStore::Controller& controller) {
    CHECK(controller);
    controller_ = controller;
  }

  void set_updater(const KVStore::Updater& updater)  {
    CHECK(updater);
    updater_ = updater;
  }

  /**
   * \brief blocked until received the command \a kSyncMode
   */
  void Run() {
    exec_.Start();
  }

 private:
  struct UpdateBuf {
    std::vector<ps::KVMeta> request;
    NDArray merged;
    // temp_array is used to cast received values as float32 for computation if required
    NDArray temp_array;
  };
  std::unordered_map<int, ps::KVPairs<char>> req_data_buf;
  std::unordered_map<int, ps::KVMeta> req_meta_buf;
  std::queue<int> send_q;
  std::unordered_map<int, UpdateBuf> update_buf_global;
  std::mutex send_mu;

  void WorkersMerge(const ps::KVMeta& req_meta,
                    const ps::KVPairs<char>& req_data,
                    ps::KVServer<char>* server) {
    // Send my data to other servers.
    if (req_meta.num_merge == -1) {
      int key = send_q.front();

      // Copy my data to the sender buffer.
      req_data_buf[key].vals.CopyFrom(
        static_cast<const char*>(update_buf_global[key].merged.data().dptr_),
        req_data_buf[key].lens[0]
      );
      update_buf_global[key].merged.WaitToRead();

      // Send the data.
      ps_server_->Send(
        req_meta_buf[key].timestamp,
        req_meta_buf[key].push,
        req_meta_buf[key].cmd,
        req_data_buf[key],
        req_meta_buf[key].key,
        req_meta_buf[key].version,
        req_meta_buf[key].app_id,
        req_meta_buf[key].customer_id,
        req_meta_buf[key].num_merge);

      std::unique_lock <std::mutex> send_lk(send_mu);
      send_q.pop();
      send_lk.unlock();
    } else {
      // Receive data from other servers and merge them.
      CHECK_EQ(req_data.keys.size(), (size_t) 1);
      if (req_meta.push) {
        CHECK_EQ(req_data.lens.size(), (size_t) 1);
        CHECK_EQ(req_data.vals.size(), (size_t) req_data.lens[0]);
      }

      int key = req_data.keys[0];
      int my_id = ps::Postoffice::Get()->van()->my_node_global_.id;
      DataHandleType type = DepairDataHandleType(req_meta.cmd);

      std::unique_lock <std::mutex> send_lk(send_mu);
      if (req_meta.sender == my_id) {
        send_q.push(key);
        req_meta_buf[key].cmd = req_meta.cmd;
        req_meta_buf[key].push = req_meta.push;
        req_meta_buf[key].sender = req_meta.sender;
        req_meta_buf[key].timestamp = req_meta.timestamp;
        req_meta_buf[key].customer_id = req_meta.customer_id;
        req_meta_buf[key].app_id = req_meta.app_id;
        req_meta_buf[key].key = req_meta.key;
        req_meta_buf[key].version = req_meta.version;
        req_meta_buf[key].num_merge = req_meta.num_merge;
        req_data_buf[key].keys = req_data.keys;
        req_data_buf[key].lens = req_data.lens;
      } else {
        server->Response(req_meta, true);
      }

      size_t ds[] = {(size_t) req_data.lens[0] / mshadow::mshadow_sizeof(type.dtype)};
      TShape dshape(ds, ds + 1);
      TBlob recv_blob;
      MSHADOW_REAL_TYPE_SWITCH(type.dtype, DType, {
          recv_blob = TBlob(reinterpret_cast<DType *>(req_data.vals.data()), dshape, cpu::kDevMask);
      })
      NDArray recved = NDArray(recv_blob, 0);

      auto &updates = update_buf_global[key];
      if (updates.merged.is_none()) {
        updates.merged = NDArray(dshape, Context(), false, type.dtype);
      }

      if (req_meta.sender == my_id) {
        updates.merged = recved;
        updates.merged.WaitToRead();
      } else {
        updates.merged += recved;
        updates.merged.WaitToRead();
        req_meta_buf[key].num_merge += req_meta.num_merge;
      }
      send_lk.unlock();
    }
  }

  void CommandHandle(const ps::SimpleData& recved, ps::SimpleApp* app) {
    CommandType recved_type = static_cast<CommandType>(recved.head);
    switch (recved_type) {
      case CommandType::kStopServer:
        if (ps::IsGlobalServer()) {
          if (++num_stop_executor_ == ps::NumGlobalWorkers()) {
            LOG(INFO) << "Stop executor";
            exec_.Stop();
          }
        } else {
          CHECK_NOTNULL(ps_server_);
          int cmd_id = static_cast<int>(CommandType::kStopServer);
          ps_server_->Request(cmd_id, "", ps::kServerGroupGlobal, true);
          LOG(INFO) << "Stop executor";
          exec_.Stop();
        }
        break;
      case CommandType::kSyncMode:
        sync_mode_ = true;
        break;
      case CommandType::kSyncGlobalMode:
        CHECK(ps::IsGlobalServer())
          << "only global server could perform global synchronous mode";
        sync_global_mode_ = true;
        break;
      case CommandType::kSetGradientCompression:
        gradient_compression_->DecodeParams(recved.body);
        if (ps::IsGlobalServer() && ps::MyRank(true) == 0) {
          int cmd_id = static_cast<int>(CommandType::kSetGradientCompression);
          ps_server_->Request(cmd_id, recved.body, ps::kWorkerGroupGlobal, true);
          ps_server_->Request(cmd_id, recved.body, ps::kWorkerGroup, false);
        }
        break;
      case CommandType::kSetProfilerParams:
        // last char is the type of profiler command
        ProcessServerProfilerCommands(static_cast<KVStoreServerProfilerCommand>
          (recved.body.back() - '0'), recved.body);
        break;
      case CommandType::kSetMultiPrecision:
        // uses value 1 for message id from frontend
        if (!multi_precision_) {
          multi_precision_ = true;
          CreateMultiPrecisionCopies();
        }
        break;
      case CommandType::kController:
        // this uses value 0 for message id from frontend
        // let the main thread to execute ctrl, which is necessary for python
        exec_.Exec([this, recved]() {
          CHECK(controller_);
          controller_(recved.head, recved.body);
        });
        break;
    }
    app->Response(recved);
  }

  /*
   * For keys already initialized, if necessary create stored_realt.
   * This will only be used if by some wrong usage of kvstore,
   * some keys are initialized before optimizer is set.
   */
  void CreateMultiPrecisionCopies() {
    for (auto const &stored_entry : store_) {
      const int key = stored_entry.first;
      const NDArray &stored = stored_entry.second;
      if (stored.dtype() != mshadow::kFloat32) {
        auto &stored_realt = store_realt_[key];
        if (stored.storage_type() == kRowSparseStorage) {
          stored_realt = NDArray(
            kRowSparseStorage, stored.shape(), stored.ctx(), true, mshadow::kFloat32);
        } else {
          stored_realt = NDArray(
            stored.shape(), stored.ctx(), false, mshadow::kFloat32);
        }

        auto &update = update_buf_[key];
        if (!update.merged.is_none()) {
          if (update.merged.storage_type() == kRowSparseStorage) {
            update.merged = NDArray(
              kRowSparseStorage, update.merged.shape(), update.merged.ctx(), true, mshadow::kFloat32);
          } else {
            update.merged = NDArray(
              update.merged.shape(), update.merged.ctx(), false, mshadow::kFloat32);
          }
        }
        CHECK(update.request.size() == 0)
          << ps::MyRank() << "Multiprecision mode can not be set while pushes are underway."
          << "Please set optimizer before pushing keys." << key << " " << update.request.size();
        CopyFromTo(stored, stored_realt);
      }
    }
    for (auto const &stored_realt_entry : store_realt_) {
      stored_realt_entry.second.WaitToRead();
    }
  }

  void ProcessServerProfilerCommands(KVStoreServerProfilerCommand type, const std::string& body) {
    switch (type) {
      case KVStoreServerProfilerCommand::kSetConfig:
        SetProfilerConfig(body.substr(0, body.size() - 1));
        break;
      case KVStoreServerProfilerCommand::kState:
        MXSetProfilerState(static_cast<int>(body.front() - '0'));
        break;
      case KVStoreServerProfilerCommand::kPause:
        MXProfilePause(static_cast<int>(body.front() - '0'));
        break;
      case KVStoreServerProfilerCommand::kDump:
        MXDumpProfile(static_cast<int>(body.front() - '0'));
        break;
    }
  }

  void SetProfilerConfig(std::string params_str) {
    std::vector<std::string> elems;
    mxnet::kvstore::split(params_str, ',', std::back_inserter(elems));
    std::vector<const char*> ckeys;
    std::vector<const char*> cvals;
    ckeys.reserve(elems.size());
    cvals.reserve(elems.size());

    for (size_t i=0; i < elems.size(); i++) {
      std::vector<std::string> parts;
      mxnet::kvstore::split(elems[i], ':', std::back_inserter(parts));
      CHECK_EQ(parts.size(), 2) << "Improper profiler config passed from worker";
      CHECK(!parts[0].empty()) << "ProfilerConfig parameter is empty";
      CHECK(!parts[1].empty()) << "ProfilerConfig value is empty for parameter "<< parts[0];
      if (parts[0] == "filename") {
        parts[1] = "rank" + std::to_string(ps::MyRank()) + "_" + parts[1];
      }
      char* ckey = new char[parts[0].length() + 1];
      std::snprintf(ckey, parts[0].length() + 1, "%s", parts[0].c_str());
      ckeys.push_back(ckey);

      char* cval = new char[parts[1].length() + 1];
      std::snprintf(cval, parts[1].length() + 1, "%s", parts[1].c_str());
      cvals.push_back(cval);
    }
    MXSetProfilerConfig(elems.size(), &ckeys[0], &cvals[0]);
    for (size_t i=0; i < ckeys.size(); i++) {
      delete[] ckeys[i];
      delete[] cvals[i];
    }
  }

  void DataHandleEx(const ps::KVMeta& req_meta,
                    const ps::KVPairs<char>& req_data,
                    ps::KVServer<char>* server) {
    DataHandleType type = DepairDataHandleType(req_meta.cmd);

    switch (type.requestType) {

      case RequestType::kRowSparsePushPull:
        DataHandleRowSparse(type, req_meta, req_data, server);
        break;

      case RequestType::kCompressedPushPull:
        if (req_meta.push) {
          const bool request = req_meta.sender % 2 == 1;
          if (request) {
            if (ps::IsGlobalServer() && !sync_global_mode_) {
              DataHandleAsyncCompressed(type, req_meta, req_data, server);
            } else {
              DataHandleSyncCompressed(type, req_meta, req_data, server);
            }
          } else {
            DataHandlePushResponseDefault(type, req_meta, server);
          }
        } else {
          DataHandlePullDefault(type, req_meta, req_data, server);
        }
        break;

      case RequestType::kDefaultPushPull:
        if (req_meta.push) {
          const bool request = (ps_server_->enable_inter_ts) ?
            req_meta.app_id : req_meta.sender % 2 == 1;
          if (request) {
            if (ps::IsGlobalServer() && !sync_global_mode_) {
              DataHandleAsyncDefault(type, req_meta, req_data, server);
            } else {
              DataHandleSyncDefault(type, req_meta, req_data, server);
            }
          } else {
            DataHandlePushResponseDefault(type, req_meta, server);
          }
        } else {
          DataHandlePullDefault(type, req_meta, req_data, server);
        }
        break;

      case RequestType::kBSCompressedPushPull:
        if (req_meta.push) {
          const bool request = req_meta.sender % 2 == 1;
          if (request) {
            if (ps::IsGlobalServer() && !sync_global_mode_) {
              DataHandleAsyncBSCompressed(type, req_meta, req_data, server);
            } else {
              DataHandleSyncBSCompressed(type, req_meta, req_data, server);
            }
          } else {
            DataHandlePushResponseDefault(type, req_meta, server);
          }
        } else {
          DataHandlePullDefault(type, req_meta, req_data, server);
        }
        break;

      default :
        LOG(FATAL) << "Unsupported RequestType";
    }
  }

  inline bool has_multi_precision_copy(const DataHandleType type) {
    return multi_precision_ && type.dtype != mshadow::kFloat32;
  }

  inline void ApplyUpdates(const DataHandleType type, const int key,
                           UpdateBuf *update_buf, ps::KVServer<char>* server) {
    ApplyUpdates(type, key, true, update_buf, server);
  }

  inline void ApplyUpdates(const DataHandleType type, const int key, const bool sync_mode,
                           UpdateBuf *update_buf, ps::KVServer<char>* server,
                           const bool enable_ts = false, const ps::KVMeta& req_meta = ps::KVMeta(),
                           const ps::KVPairs<char>& req_data = ps::KVPairs<char>()) {
    // let the main thread to execute updater_, which is necessary for python
    auto& stored = has_multi_precision_copy(type) ? store_realt_[key] : store_[key];
    auto& update = sync_mode ? update_buf->merged : update_buf->temp_array;
    if (updater_ && ps::IsGlobalServer()) {
      CHECK(updater_);
      exec_.Exec([this, key, &update, &stored]() {
        updater_(key, update, &stored);
      });
    } else {
      CHECK(sync_mode);
      CopyFromTo(update_buf->merged, &stored);
    }
    if (has_multi_precision_copy(type)) {
      CopyFromTo(stored, store_[key]);
    }
    stored.WaitToRead();
    if (enable_ts) {
      // Send the aggregated data to other servers.
      DefaultAutoPull(type, key, store_v_[key], req_meta, req_data, server, true);
    }
  }

  void DecodeRowIds(const ps::SArray<ps::Key> &keys, int64_t *indices,
                    const int64_t master_key, const int64_t num_rows) {
    indices[0] = 0;
    for (int64_t i = 1; i <= num_rows; i++) {
      int key = DecodeKey(keys[i]);
      auto row_id = key - master_key;
      indices[i - 1] = row_id;
    }
  }

  void AccumulateRowSparseGrads(const DataHandleType type, const NDArray& recved,
                                UpdateBuf* updateBuf) {
    NDArray out(kRowSparseStorage, updateBuf->merged.shape(), Context(), true,
                has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype);
    if (has_multi_precision_copy(type)) CopyFromTo(recved, updateBuf->temp_array);
    const NDArray& to_merge = has_multi_precision_copy(type) ? updateBuf->temp_array : recved;
    // accumulate row_sparse gradients
    using namespace mshadow;
    Engine::Get()->PushAsync(
      [to_merge, updateBuf, out](RunContext ctx, Engine::CallbackOnComplete on_complete) {
        op::ElemwiseBinaryOp::ComputeEx<cpu, op::mshadow_op::plus>(
        {}, {}, {to_merge, updateBuf->merged}, {kWriteTo}, {out});
        on_complete();
      }, to_merge.ctx(), {to_merge.var(), updateBuf->merged.var()}, {out.var()},
      FnProperty::kNormal, 0, PROFILER_MESSAGE_FUNCNAME
    );
    CopyFromTo(out, &(updateBuf->merged), 0);
    updateBuf->merged.WaitToRead();
  }

  void RowSparsePullResponse(const DataHandleType type, const int master_key, const size_t num_rows,
                             const ps::KVMeta& req_meta, const ps::KVPairs<char>& req_data,
                             ps::KVServer<char>* server) {
    ps::KVPairs<char> response;
    if (num_rows == 0) {
      std::vector<int> lens(req_data.keys.size(), 0);
      response.keys = req_data.keys;
      response.lens.CopyFrom(lens.begin(), lens.end());
      server->Response(req_meta, response);
      return;
    }
    const NDArray& stored = store_[master_key];
    if (has_multi_precision_copy(type)) stored.WaitToRead();
    CHECK(!stored.is_none()) << "Init " << master_key << " first";
    auto shape = stored.shape();
    auto unit_len = shape.ProdShape(1, shape.ndim());
    const int num_bytes = mshadow::mshadow_sizeof(type.dtype);
    const int unit_size = unit_len * num_bytes;
    const char* data = static_cast<char *> (stored.data().dptr_);
    auto len = num_rows * unit_size;
    // concat values
    response.vals.resize(len);
    #pragma omp parallel for
    for (size_t i = 1; i <= num_rows; i++) {
      int key = DecodeKey(req_data.keys[i]);
      int64_t row_id = key - master_key;
      const auto src = data + row_id * unit_size;
      auto begin = (i - 1) * unit_size;
      auto end = i * unit_size;
      response.vals.segment(begin, end).CopyFrom(src, unit_size);
    }
    // setup response
    response.keys = req_data.keys;
    std::vector<int> lens(req_data.keys.size(), unit_len);
    lens[0] = 0;
    response.lens.CopyFrom(lens.begin(), lens.end());
    server->Response(req_meta, response);
  }

  void InitRowSparseStored(const DataHandleType type, const int master_key, const size_t num_rows,
                           const ps::KVMeta& req_meta, const ps::KVPairs<char>& req_data,
                           ps::KVServer<char>* server) {
    auto& stored = has_multi_precision_copy(type) ? store_realt_[master_key] : store_[master_key];
    int dtype = type.dtype;
    int num_bytes = mshadow::mshadow_sizeof(dtype);
    auto unit_len = req_data.lens[1] / num_bytes;
    CHECK_GT(unit_len, 0);
    size_t ds[] = {num_rows, (size_t) unit_len};
    TShape dshape(ds, ds + 2);
    CHECK_EQ(req_data.vals.size(), num_rows * unit_len * num_bytes);
    TBlob recv_blob;
    MSHADOW_REAL_TYPE_SWITCH(dtype, DType, {
      recv_blob = TBlob(reinterpret_cast<DType*>(req_data.vals.data()), dshape, cpu::kDevMask);
    })
    NDArray recved = NDArray(recv_blob, 0);
    stored = NDArray(
      kRowSparseStorage, dshape, Context(), true,
      has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype);
    if (has_multi_precision_copy(type)) {
      store_[master_key] = NDArray(
        kRowSparseStorage, dshape, Context(), true, type.dtype);
    }
    Engine::Get()->PushAsync(
      [this, recved, stored, type](RunContext ctx, Engine::CallbackOnComplete on_complete) {
        NDArray rsp = stored;
        stored.CheckAndAlloc({mshadow::Shape1(recved.shape()[0])});
        mshadow::Stream<cpu> *s = ctx.get_stream<cpu>();
        using namespace mxnet::op;
        nnvm::dim_t nnr = rsp.shape()[0];
        MSHADOW_IDX_TYPE_SWITCH(rsp.aux_type(rowsparse::kIdx), IType, {
          IType* idx = rsp.aux_data(rowsparse::kIdx).dptr<IType>();
          mxnet_op::Kernel<PopulateFullIdxRspKernel, cpu>::Launch(s, nnr, idx);
        });
        TBlob rsp_data = rsp.data();
        // copies or casts as appropriate
        ndarray::Copy<cpu, cpu>(recved.data(), &rsp_data, Context(), Context(), RunContext());
        on_complete();
      }, recved.ctx(), {recved.var()}, {stored.var()},
      FnProperty::kNormal, 0, PROFILER_MESSAGE_FUNCNAME
    );
    if (has_multi_precision_copy(type)) {
      CopyFromTo(stored, store_[master_key]);
      store_[master_key].WaitToRead();
    }
    stored.WaitToRead();
    server->Response(req_meta);
  }

  void DataHandleRowSparse(const DataHandleType type, const ps::KVMeta& req_meta,
                           const ps::KVPairs<char>& req_data, ps::KVServer<char>* server) {
    int master_key = DecodeKey(req_data.keys[0]);
    auto num_rows = req_data.keys.size() - 1;
    auto& stored = store_[master_key];
    if (req_meta.push) {
      CHECK_GT(req_data.lens.size(), 0) << "req_data.lens cannot be empty";
      CHECK_EQ(req_data.lens[0], 0);
      if (stored.is_none()) {
        // initialization
        CHECK_GT(num_rows, 0) << "Init with empty data is not supported";
        InitRowSparseStored(type, master_key, num_rows, req_meta, req_data, server);
        return;
      } else {
        auto& updates = update_buf_[master_key];
        if (sync_mode_ && updates.merged.is_none()) {
          updates.merged = NDArray(
            kRowSparseStorage, stored.shape(), Context(), true,
            has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype);
        }
        if (has_multi_precision_copy(type) && updates.temp_array.is_none()) {
          updates.temp_array = NDArray(
            kRowSparseStorage, stored.shape(), Context(), false, mshadow::kFloat32);
        }
        if (num_rows == 0) {
          if (sync_mode_) {
            if (updates.request.empty()) {
              // reset to zeros
              int merged_dtype = has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype;
              updates.merged = NDArray(kRowSparseStorage, stored.shape(), Context(), true, merged_dtype);
            }  // else nothing to aggregate
            updates.request.push_back(req_meta);
            ApplyUpdates(type, master_key, &updates, server);
          } else {
            server->Response(req_meta);
          }
        } else {
          auto unit_len = req_data.lens[1] / mshadow::mshadow_sizeof(type.dtype);
          CHECK_GT(unit_len, 0);
          // indices
          std::vector<int64_t> indices(num_rows);
          DecodeRowIds(req_data.keys, indices.data(), master_key, num_rows);

          // data
          TBlob idx_blob(indices.data(), mshadow::Shape1(num_rows), cpu::kDevMask);
          size_t ds[] = {(size_t) num_rows, (size_t) unit_len};
          TShape dshape(ds, ds + 2);
          TBlob recv_blob;
          MSHADOW_REAL_TYPE_SWITCH(type.dtype, DType, {
            recv_blob = TBlob(
              reinterpret_cast<DType*>(req_data.vals.data()), dshape, cpu::kDevMask);
          });
          // row_sparse NDArray
          NDArray recved(kRowSparseStorage, stored.shape(), recv_blob, {idx_blob}, 0);

          if (updates.request.empty()) {
            if (sync_mode_) {
              CopyFromTo(recved, updates.merged);
            } else {
              if (has_multi_precision_copy(type)) {
                CopyFromTo(recved, updates.temp_array);
              } else {
                updates.temp_array = recved;
              }
            }
          } else {
            CHECK(sync_mode_);
            AccumulateRowSparseGrads(type, recved, &updates);
          }
          updates.request.push_back(req_meta);
          ApplyUpdates(type, master_key, &updates, server);
        }
      }
    } else {
      // pull
      RowSparsePullResponse(type, master_key, num_rows, req_meta, req_data, server);
    }
  }

  int DataPushToGlobalServersDefault(const DataHandleType type, const int key,
                                     ps::KVServer<char>* server) {
    CHECK(!ps::IsGlobalServer()) << "Invalid push operation on global servers";
    CHECK(gradient_compression_->get_type() == CompressionType::kNone);

    const auto& stored = has_multi_precision_copy(type) ? store_realt_[key] : store_[key];
    CHECK(!stored.is_none()) << "Init " << key << " first";

    // as server returns when store_realt is ready in this case
    if (has_multi_precision_copy(type)) stored.WaitToRead();

    const int dtype = type.dtype;
    const int num_bytes = mshadow::mshadow_sizeof(dtype);
    const int num_arr_elems = stored.shape().Size();
    const size_t size = num_arr_elems * num_bytes;
    const int cmd = GetCommandType(RequestType::kDefaultPushPull, dtype);

    PSKV& pskv =  EncodeDefaultKey(key, stored.shape().Size(), num_bytes);
    ps::KVPairs<char> params;
    char *data = static_cast<char *>(stored.data().dptr_);
    ps::SArray<char> vals(data, size, false);
    params.keys = pskv.keys;
    params.lens = pskv.lens;
    params.vals = vals;
    return server->Push(params, cmd, nullptr, key);
  }

  int DataPushToGlobalServersCompressed(const DataHandleType type, const int key,
                                        ps::KVServer<char>* server) {
    CHECK(!ps::IsGlobalServer()) << "Invalid push operation on global servers";
    CHECK(gradient_compression_->get_type() == CompressionType::kTwoBit);
    CHECK_EQ(type.dtype, mshadow::kFloat32) << "Gradient compression is only supported for "
                                            << "float32 type of parameters";

    const auto& stored = store_[key];
    CHECK(!stored.is_none()) << "Init " << key << " first";

    const int dtype = stored.dtype();
    const int num_bytes = mshadow::mshadow_sizeof(dtype);
    const int original_size = stored.shape().Size();
    const int cmd = GetCommandType(RequestType::kCompressedPushPull, dtype);

    PSKV& pskv = EncodeCompressedKey(key, original_size, true, num_bytes);

    auto& small_buf = compr_buf_[key];
    auto& res_buf = residual_[key];

    if (small_buf.is_none()) {
      small_buf = NDArray(
        mxnet::TShape{pskv.size}, stored.ctx(), false, dtype);
      res_buf = NDArray(
        mxnet::TShape{static_cast<int64_t>(original_size)}, stored.ctx(), false, dtype);
      res_buf = 0;
    }

    gradient_compression_->Quantize(stored, &small_buf, &res_buf, 0);

    ps::KVPairs<char> compr_params;
    size_t size = small_buf.shape().Size() * mshadow::mshadow_sizeof(dtype);
    char* data = static_cast<char*>(small_buf.data().dptr_);
    ps::SArray<char> vals(data, size, false);
    compr_params.keys = pskv.keys;
    compr_params.lens = pskv.lens;
    compr_params.vals = vals;
    int ts = server->Push(compr_params, cmd);
    return ts;
  }

  int DataPushToGlobalServersBSCompressed(const DataHandleType type, const int key,
                                          ps::KVServer<char>* server) {
    CHECK(!ps::IsGlobalServer()) << "Invalid push operation on global servers";
    CHECK(gradient_compression_->get_type() == CompressionType::kBiSparseCompression);

    const auto &stored = has_multi_precision_copy(type) ? store_realt_[key] : store_[key];
    CHECK(!stored.is_none()) << "Init " << key << " first";

    // as server returns when store_realt is ready in this case.
    if (has_multi_precision_copy(type)) stored.WaitToRead();

    const unsigned int original_size = stored.shape().Size();

    // compress the data if the size is above the threshold.
    if (original_size >= size_lower_bound) {
      const int dtype = mshadow::kFloat32;
      const int num_bytes = mshadow::mshadow_sizeof(dtype);
      const int cmd = GetCommandType(RequestType::kBSCompressedPushPull, dtype);
      PSKV &pskv = EncodeBSCompressedKey(key, original_size, true, num_bytes);

      // Calculate the expected size of the compressed data.
      float threshold = gradient_compression_->get_threshold();
      const int zipped_size = float(original_size) * threshold * 2;
      
      auto &small_buf = compr_buf_[key];
      auto &velocity = velocity_[key];
      auto &accumulated_velocity = accumulated_velocity_[key];

      if (small_buf.is_none()) {
        small_buf = NDArray(
          mxnet::TShape{static_cast<int64_t>(zipped_size)}, stored.ctx(), false, dtype);
        velocity = NDArray(
          mxnet::TShape{static_cast<int64_t>(original_size)}, stored.ctx(), false, mshadow::kFloat32);
        accumulated_velocity = NDArray(
          mxnet::TShape{static_cast<int64_t>(original_size)}, stored.ctx(), false, mshadow::kFloat32);
        velocity = 0;  // Initialize with zeros.
        accumulated_velocity = 0;  // Initialize with zeros.
      }

      // Compress the data.
      gradient_compression_->BSCompress(stored, small_buf, velocity, accumulated_velocity, 0);

      // Prepare compressed data for sending.
      ps::KVPairs<char> compr_params;
      size_t size = small_buf.shape().Size() * mshadow::mshadow_sizeof(dtype);
      char* data = static_cast<char *>(small_buf.data().dptr_);
      ps::SArray<char> vals(data, size, false);
      compr_params.keys = pskv.keys;
      compr_params.lens = pskv.lens;
      compr_params.vals = vals;
      return server->Push(compr_params, cmd, nullptr, key);
    } else {
      // Data with size smaller than the threshold does not need to be compressed, send it as is
      const int dtype = type.dtype;
      const int num_bytes = mshadow::mshadow_sizeof(dtype);
      const int num_arr_elems = stored.shape().Size();
      const size_t size = num_arr_elems * num_bytes;
      const int cmd = GetCommandType(RequestType::kDefaultPushPull, dtype);

      PSKV &pskv = EncodeDefaultKey(key, stored.shape().Size(), num_bytes);

      ps::KVPairs<char> params;
      char* data = static_cast<char*>(stored.data().dptr_);
      ps::SArray<char> vals(data, size, false);
      params.keys = pskv.keys;
      params.lens = pskv.lens;
      params.vals = vals;
      return server->Push(params, cmd, nullptr, key);
    }
  }

  void DataPullFromGlobalServersDefault(const DataHandleType type, const int key,
                                        ps::KVServer<char>* server) {
    CHECK(!ps::IsGlobalServer()) << "Invalid pull operation on global servers";

    // Determine the type of compression used.
    const bool is_compressed = gradient_compression_->get_type() == CompressionType::kTwoBit;
    const bool is_bscompressed = type.requestType == RequestType::kBSCompressedPushPull;

    auto &stored = (is_compressed || !has_multi_precision_copy(type)
      || !is_bscompressed) ? store_[key] : store_realt_[key];

    const size_t num_arr_elems = stored.shape().Size();
    const int dtype = type.dtype;
    const int num_bytes = mshadow::mshadow_sizeof(dtype);

    // Determine the command type.
    int cmd; PSKV pskv = PSKV();
    if (is_compressed) {
      cmd = GetCommandType(RequestType::kCompressedPushPull, dtype);
      pskv = EncodeCompressedKey(key, num_arr_elems, false, num_bytes);
    } else if (is_bscompressed) {
      cmd = GetCommandType(RequestType::kBSCompressedPushPull, dtype);
      pskv = EncodeBSCompressedKey(key, num_arr_elems, false, num_bytes);
    } else {
      cmd = GetCommandType(RequestType::kDefaultPushPull, dtype);
      pskv = EncodeDefaultKey(key, num_arr_elems, num_bytes);
    }

    mu_.lock();
    for (auto& ps_key : pskv.keys)
      key_map_[ps_key] = key;
    mu_.unlock();

    // pull latest params from global servers.
    if (!ps_server_->enable_inter_ts || !initialized_[key]) {
      server->Pull(pskv.keys, cmd, key);
    }
  }

  /**
   * Handles the response after a push operation to global servers.
   */
  void DataHandlePushResponseDefault(const DataHandleType type, const ps::KVMeta& req_meta,
                                     ps::KVServer<char>* server) {
    CHECK(req_meta.push);
    CHECK(!ps::IsGlobalServer()) << "Invalid push response on global servers";
    const int ts = req_meta.timestamp;

    // Exit if responses haven't been received from all global servers.
    if (server->NumResponse(ts) != ps::NumGlobalServers() - 1) return;

    mu_.lock();
    int key = ts_key_map_[ts];
    ts_key_map_.erase(ts);
    mu_.unlock();

    // Pull latest data from the global servers.
    DataPullFromGlobalServersDefault(type, key, server);
  }

  void HandleHFAAccumulate(const DataHandleType& type, NDArray& stored,
                           NDArray& stored_milestone, const NDArray& recved) {
    CHECK(use_hfa) << "Invalid operation, hfa is not enabled";
    if (stored_milestone.is_none()) {
      stored_milestone = NDArray(stored.shape(), Context(), false, type.dtype);
      stored.WaitToRead();
      CopyFromTo(stored, stored_milestone, 0);
    } else {
      stored = stored_milestone + recved;
      stored.WaitToRead();
      stored_milestone = stored;
      stored_milestone.WaitToRead();
    }
  }

  void DataHandlePullResponseDefault(const DataHandleType type, const ps::KVMeta& req_meta,
                                     const ps::KVPairs<char>& req_data, ps::KVServer<char>* server) {
    CHECK(!ps::IsGlobalServer()) << "Invalid pull operation on global servers";
    const bool is_default = type.requestType == RequestType::kDefaultPushPull;
    const bool is_compressed = type.requestType == RequestType::kCompressedPushPull;
    const bool is_bscompressed = type.requestType == RequestType::kBSCompressedPushPull;
    if (is_compressed)
      CHECK_EQ(type.dtype, mshadow::kFloat32)
        << "Gradient compression is currently supported for fp32 only";

    mu_.lock();
    int key = key_map_[req_data.keys[0]];
    mu_.unlock();

    mu_.lock();
    auto& kvs = recv_kvs_[key];
    mu_.unlock();
    kvs.push_back(req_data);

    const int num_parts = kvs.size();
    auto& stored = (is_compressed || !has_multi_precision_copy(type)) ? store_[key] : store_realt_[key];
    auto& stored_milestone = store_milestone_[key];
    const size_t num_arr_elems = stored.shape().Size();
    const int num_bytes = mshadow::mshadow_sizeof(type.dtype);

    if (num_arr_elems >= bigarray_bound_
      && num_parts != ps::NumGlobalServers()) return;

    // Handle small data tensor.
    if (num_arr_elems < bigarray_bound_) {
      size_t ds[] = {(size_t) req_data.lens[0] / num_bytes};
      TShape dshape(ds, ds + 1);
      TBlob recv_blob;
      MSHADOW_REAL_TYPE_SWITCH(type.dtype, DType, {
        recv_blob = TBlob(reinterpret_cast<DType*>(req_data.vals.data()), dshape, cpu::kDevMask);
      });
      NDArray recved = NDArray(recv_blob, 0);
      if (is_default || is_compressed) {
        if (use_hfa) {
          HandleHFAAccumulate(type, stored, stored_milestone, recved);
        } else {
          CopyFromTo(recved, &stored, 0);
        }
      } else if (is_bscompressed) {
        const int original_size = stored.shape().Size();
        NDArray temp_array = NDArray(
          mxnet::TShape{static_cast<int64_t>(original_size)}, stored.ctx(), false, type.dtype);
        temp_array = 0;  // Initialize with all zeros.
        gradient_compression_->BSCDecompress(recved, temp_array, 0);
        temp_array.WaitToRead();
        if (use_hfa) {
          HandleHFAAccumulate(type, stored, stored_milestone, temp_array);
        } else {
          CopyFromTo(temp_array, &stored);
        }
      }
      stored.WaitToRead();
      if (!is_compressed && has_multi_precision_copy(type)) {
        auto& stored_dtype = store_[key];
        stored_dtype = NDArray(dshape, Context(), false, type.dtype);
        CopyFromTo(stored, stored_dtype);
        stored_dtype.WaitToRead();
      }
    } else {
      // Handle large data tensor.
      PSKV& pskv =
        is_compressed ? EncodeCompressedKey(key, num_arr_elems, false, num_bytes) :
        is_bscompressed ? EncodeBSCompressedKey(key, num_arr_elems, false, num_bytes) :
        EncodeDefaultKey(key, num_arr_elems, num_bytes);
      auto& keys = pskv.keys;
      auto* lens = &pskv.lens;
      auto& recv_buf = comm_buf_[key];
      if (recv_buf.is_none()) {
        recv_buf = NDArray(stored.shape(), stored.ctx(), false, type.dtype);
      }
      char* data = static_cast<char*>(recv_buf.data().dptr_);
      // false means not to delete data when SArray is deleted
      auto vals = new ps::SArray<char>(data, num_arr_elems * num_bytes, false);

      // do check
      CHECK_NOTNULL(vals);
      size_t total_key = 0, total_val = 0;
      for (const auto& s : kvs) {
        ps::Range range = FindRange(keys, s.keys.front(), s.keys.back() + 1);
        CHECK_EQ(range.size(), s.keys.size()) << "Unmatched keys size from one server";
        if (lens) CHECK_EQ(s.lens.size(), s.keys.size());
        total_key += s.keys.size();
        total_val += s.vals.size();
      }
      CHECK_EQ(total_key, keys.size()) << "Lost some servers?";

      // uniformly partitioned to all global servers
      // fill vals and lens
      std::sort(kvs.begin(), kvs.end(), [](
        const ps::KVPairs<char>& a, const ps::KVPairs<char>& b) {
        return a.keys.front() < b.keys.front();
      });
      if (vals->empty()) {
        vals->resize(total_val);
      } else {
        if (!is_bscompressed) {
          CHECK_EQ(vals->size(), total_val);
        }
      }
      char* p_vals = vals->data();
      int* p_lens = nullptr;
      if (lens) {
        if (lens->empty()) {
          lens->resize(keys.size());
        } else {
          CHECK_EQ(lens->size(), keys.size());
        }
        p_lens = lens->data();
      }
      for (const auto& s : kvs) {
        memcpy(p_vals, s.vals.data(), s.vals.size() * sizeof(char));
        p_vals += s.vals.size();
        if (p_lens) {
          memcpy(p_lens, s.lens.data(), s.lens.size() * sizeof(int));
          p_lens += s.lens.size();
        }
      }
      if (is_default || is_compressed) {
        if (use_hfa) {
          HandleHFAAccumulate(type, stored, stored_milestone, recv_buf);
        } else {
          CopyFromTo(recv_buf, &stored, 0);
        }
      } else if (is_bscompressed) {
        const int original_size = stored.shape().Size();
        NDArray temp_array = NDArray(
          mxnet::TShape{static_cast<int64_t>(original_size)}, stored.ctx(), false, type.dtype);
        temp_array = 0;
        gradient_compression_->BSCDecompress(recv_buf, temp_array, 0);
        temp_array.WaitToRead();
        if (use_hfa) {
          HandleHFAAccumulate(type, stored, stored_milestone, temp_array);
        } else {
          CopyFromTo(temp_array, &stored);
        }
      }
      stored.WaitToRead();
      if (!is_compressed && has_multi_precision_copy(type)) {
        auto &stored_dtype = store_[key];
        stored_dtype = NDArray(stored.shape(), stored.ctx(), false, type.dtype);
        CopyFromTo(stored, stored_dtype);
        stored_dtype.WaitToRead();
      }
    }
    mu_.lock();
    recv_kvs_.erase(key);
    mu_.unlock();

    mu_.lock();
    auto& init = initialized_[key];
    mu_.unlock();
    if (!init.load()) {
      // active the first pull operation
      mu_.lock();
      initialized_[key] = true;
      mu_.unlock();
      if (ps_server_->enable_intra_ts) {
        DefaultAutoPull(type, key, store_v_[key], req_meta, req_data, server, false);
      }
    } else {
      // notify workers to pull
      if (ps_server_->enable_intra_ts) {
        mu_.lock();
        auto& updates_tmp = update_buf_tmp_[key];
        mu_.unlock();
        DefaultAutoPull(type, key, store_v_[key], updates_tmp.request[0], req_data, server, false);
        updates_tmp.request.clear();
      } else {
        mu_.lock();
        auto& updates_tmp = update_buf_tmp_[key];
        mu_.unlock();
        if (!ps_server_->enable_p3) {
          for (const auto& req : updates_tmp.request) {
            server->Response(req, false);
          }
        } else {
          for (const auto& req : updates_tmp.request) {
            ps::KVPairs<char> res;
            const int dtype = stored.dtype();
            size_t size = stored.shape().Size() * mshadow::mshadow_sizeof(dtype);
            char *data = static_cast<char *>(stored.data().dptr_);
            ps::SArray<char> vals(data, size, false);
            res.keys.push_back(key);
            res.vals = vals;
            res.lens.push_back(size);
            server->Response(req, res,0);
          }
        }
        updates_tmp.request.clear();
      }
    }
  }

  void DefaultStorageResponse(const DataHandleType type, const int key,
                              const ps::KVMeta& req_meta, const ps::KVPairs<char>& req_data,
                              ps::KVServer<char>* server) {
    store_mu_.lock();
    const NDArray& stored = store_[key];
    CHECK(!stored.is_none()) << "init " << key << " first";

    // as server returns when store_realt is ready in this case
    if (has_multi_precision_copy(type)) stored.WaitToRead();

    bool is_global = req_meta.sender < ps::kOffset;
    if (type.requestType == RequestType::kDefaultPushPull || 
        type.requestType == RequestType::kCompressedPushPull) {
      auto len = stored.shape().Size() * mshadow::mshadow_sizeof(type.dtype);
      ps::KVPairs<char> response;
      response.keys = req_data.keys;
      response.lens = {len};
      response.vals.CopyFrom(static_cast<const char*>(stored.data().dptr_), len);
      server->Response(req_meta, response, is_global);
    } else if (type.requestType == RequestType::kBSCompressedPushPull) {
      float threshold = gradient_compression_->get_threshold();
      const int original_size = stored.shape().Size();
      const int numWorkers = ps::NumGlobalWorkers();
      const int zipped_size = float(original_size) * threshold * numWorkers * 2;
      auto len = zipped_size * mshadow::mshadow_sizeof(type.dtype);
      NDArray small_buf = NDArray(mxnet::TShape{static_cast<int64_t>(zipped_size)}, stored.ctx(), false, type.dtype);
      NDArray stored_copy = NDArray(mxnet::TShape{static_cast<int64_t>(original_size)}, stored.ctx(), false, type.dtype);
      small_buf = 0;
      CopyFromTo(stored, stored_copy);
      gradient_compression_->BSCPullCompress(stored_copy, small_buf, numWorkers, 0);
      small_buf.WaitToRead();
      ps::KVPairs<char> response;
      response.keys = req_data.keys;
      response.lens = {len};
      response.vals.CopyFrom(static_cast<const char*>(small_buf.data().dptr_), len);
      server->Response(req_meta, response, is_global);
    } else {
      LOG(FATAL) << "Unsupported RequestType";
    }
    store_mu_.unlock();
  }

  void DataHandleSyncDefault(const DataHandleType type, const ps::KVMeta& req_meta,
                             const ps::KVPairs<char>& req_data, ps::KVServer<char>* server) {
    CHECK(req_meta.push);
    CHECK_EQ(req_data.keys.size(), (size_t)1);
    if (req_meta.push) {
      CHECK_EQ(req_data.lens.size(), (size_t)1);
      CHECK_EQ(req_data.vals.size(), (size_t)req_data.lens[0]);
    }
    if (ps::IsGlobalServer()) CHECK(sync_global_mode_);

    int key = ps_server_->enable_p3 ? (int)req_data.keys[0]:
      DecodeKey(req_data.keys[0], ps::IsGlobalServer());
    auto& stored = has_multi_precision_copy(type) ? store_realt_[key] : store_[key];
    auto& stored_milestone = store_milestone_[key];
    // there used several WaitToRead, this is because a recved's memory
    // could be deallocated when this function returns. so we need to make sure
    // the operators with a NDArray are actually finished
    size_t ds[] = {(size_t) req_data.lens[0] / mshadow::mshadow_sizeof(type.dtype)};
    TShape dshape(ds, ds + 1);
    TBlob recv_blob;
    MSHADOW_REAL_TYPE_SWITCH(type.dtype, DType, {
      recv_blob = TBlob(reinterpret_cast<DType*>(req_data.vals.data()), dshape, cpu::kDevMask);
    });
    NDArray recved = NDArray(recv_blob, 0);
    if (stored.is_none()) {
      // initialization
      stored = NDArray(dshape, Context(), false,
                       has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype);
      CopyFromTo(recved, &stored, 0);
      stored.WaitToRead();
      if (has_multi_precision_copy(type)) {
        auto &stored_dtype = store_[key];
        stored_dtype = NDArray(dshape, Context(), false, type.dtype);
        CopyFromTo(stored, stored_dtype);
        stored_dtype.WaitToRead();
      }
      if (ps::IsGlobalServer()) {
        server->Response(req_meta);
        mu_.lock();
        initialized_[key] = true;
        mu_.unlock();
      } else {
        if (!ps_server_->enable_p3) {
          server->Response(req_meta);
        } else {
          ps::KVPairs<char> res;
          const int dtype = stored.dtype();
          size_t size = stored.shape().Size() * mshadow::mshadow_sizeof(dtype);
          char *data = static_cast<char *>(stored.data().dptr_);
          ps::SArray<char> vals(data, size, false);
          res.keys.push_back(key);
          res.vals = vals;
          res.lens.push_back(size);
          server->Response(req_meta, res, 0);
        }
        DataPullFromGlobalServersDefault(type, key, server);
      }
    } else {
      // aggregate gradients from workers or servers
      CHECK(sync_mode_);
      // ignore central workers if DMLC_ENABLE_CENTRAL_WORKER is not set
      if (ps::IsGlobalServer() && req_meta.sender > ps::kOffset
        && !ps::EnableCentralWorkers()) return;

      auto &updates = update_buf_[key];
      if (updates.merged.is_none()) {
        updates.merged = NDArray(dshape, Context(), false,
                                 has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype);
      }
      if (has_multi_precision_copy(type) && updates.temp_array.is_none()) {
        updates.temp_array = NDArray(dshape, Context(), false, mshadow::kFloat32);
      }
      if (updates.request.empty()) {
        CopyFromTo(recved, updates.merged);
      } else {
        if (has_multi_precision_copy(type)) {
          CopyFromTo(recved, updates.temp_array);
          updates.temp_array.WaitToRead();
          updates.merged += updates.temp_array;
        } else {
          updates.merged += recved;
        }
      }
      updates.merged.WaitToRead();
      
      for (int i = 0; i < req_meta.num_merge; i++) {
        updates.request.push_back(req_meta);
      }

      if (ps::IsGlobalServer()) {
        int central_workers = 0;
        if (ps::EnableCentralWorkers()) central_workers = ps::NumWorkers();
        if (updates.request.size() == (size_t)central_workers + (size_t)ps::NumGlobalWorkers()) {
          // aggregate gradients and update model
          if (ps_server_->enable_inter_ts) {
            ApplyUpdates(type, key, true, &updates, server, true, req_meta, req_data);
          } else {
            ApplyUpdates(type, key, &updates, server);
          }
          // notify all workers to call pull
          for (const auto& req : updates.request) {
            server->Response(req, req.sender < ps::kOffset);
          }
          updates.request.clear();
        } else {
          updates.merged.WaitToRead();
        }
      } else {
        if (updates.request.size() == (size_t) ps::NumWorkers()) {
          // only aggregate gradients
          ApplyUpdates(type, key, &updates, server);
          if (key == 0) local_iters += 1;
          if ((local_iters % period_k2 != 0) && use_hfa) {
            for (const auto &req : updates.request) {
              server->Response(req, false);
            }
            updates.request.clear();
          } else {
            if (use_hfa) {
              CHECK(!stored_milestone.is_none()) << "init stored_milestone first!";
              stored = (stored - stored_milestone) / ps::NumGlobalWorkers();
              stored.WaitToRead();
            }
            auto &updates_tmp = update_buf_tmp_[key];
            updates_tmp.request = updates.request;
            updates.request.clear();
            if (ps_server_->enable_intra_ts) {
              for (const auto &req : updates_tmp.request) {
                server->Response(req, false);
              }
            }
            // push to global servers
            int ts;
            switch (gradient_compression_->get_type()) {
              case CompressionType::kNone:
                ts = DataPushToGlobalServersDefault(type, key, server);
                break;
              case CompressionType::kTwoBit:
                ts = DataPushToGlobalServersCompressed(type, key, server);
                break;
              case CompressionType::kBiSparseCompression:
                ts = DataPushToGlobalServersBSCompressed(type, key, server);
                break;
            }
            mu_.lock();
            ts_key_map_[ts] = key;
            mu_.unlock();
          }
        } else {
          updates.merged.WaitToRead();
        }
      }
    }
  }

  void DefaultAutoPull(const DataHandleType type, const int key, const int version,
                       const ps::KVMeta& req_meta, const ps::KVPairs<char>& req_data,
                       ps::KVServer<char>* server, bool inter_domain) {
      CHECK(type.requestType == RequestType::kDefaultPushPull);
      ps::KVPairs<char> response;
      const NDArray& stored = store_[key];
      CHECK(!stored.is_none()) << "Init " << key << " first";

      // as server returns when store_realt is ready in this case
      if (has_multi_precision_copy(type)) stored.WaitToRead();

      auto len = stored.shape().Size() * mshadow::mshadow_sizeof(stored.dtype());
      response.keys = req_data.keys;
      response.lens = {len};
      response.vals.CopyFrom(static_cast<const char*>(stored.data().dptr_), len);
      server->AutoPullUpdate(version, req_meta, response, inter_domain);
  }

  void DataHandleSyncCompressed(const DataHandleType type, const ps::KVMeta& req_meta,
                                const ps::KVPairs<char>& req_data, ps::KVServer<char>* server) {
    // do some check
    CHECK(ps::IsGlobalServer());
    CHECK(req_meta.push);
    CHECK(sync_global_mode_);
    CHECK_EQ(req_data.keys.size(), (size_t)2);
    CHECK_EQ(req_data.lens.size(), (size_t)2);
    CHECK_EQ(req_data.vals.size(), (size_t)req_data.lens[1]);
    CHECK_EQ(type.dtype, mshadow::kFloat32)
            << "Gradient compression is currently supported for fp32 only";

    int original_size = DecodeKey(req_data.keys[0], true);
    int key = DecodeKey(req_data.keys[1], true);
    auto& stored = store_[key];
    // there used several WaitToRead, this is because \a recved's memory
    // could be deallocated when this function returns. so we need to make sure
    // the operators with \a NDArray are actually finished
    size_t ds[] = {(size_t)req_data.lens[1] / mshadow::mshadow_sizeof(type.dtype)};
    TShape dshape(ds, ds + 1);
    TBlob recv_blob(reinterpret_cast<real_t*>(req_data.vals.data()), dshape, cpu::kDevMask);
    NDArray recved = NDArray(recv_blob, 0);

    NDArray decomp_buf = decomp_buf_[key];
    dshape = TShape{(int64_t) original_size};
    if (decomp_buf.is_none()) {
      decomp_buf = NDArray(dshape, Context());
    }

    if (stored.is_none()) {
      // initialization
      stored = NDArray(dshape, Context());
      gradient_compression_->Dequantize(recved, &stored, 0);
      server->Response(req_meta);
      stored.WaitToRead();
      if (ps::IsGlobalServer()) {
        mu_.lock();
        initialized_[key] = true;
        mu_.unlock();
      }
    } else {
      // aggregate gradients from central workers or servers
      // ignore central workers if DMLC_ENABLE_CENTRAL_WORKER is not set
      if (req_meta.sender > ps::kOffset && !ps::EnableCentralWorkers()) return;

      auto& updates = update_buf_[key];
      if (updates.merged.is_none()) {
        updates.merged = NDArray(dshape, Context());
      }
      if (updates.request.empty()) {
        gradient_compression_->Dequantize(recved, &updates.merged, 0);
      } else {
        gradient_compression_->Dequantize(recved, &decomp_buf, 0);
        updates.merged += decomp_buf;
      }
      updates.merged.WaitToRead();
      updates.request.push_back(req_meta);

      int central_workers = 0;
      if (ps::EnableCentralWorkers()) central_workers = ps::NumWorkers();
      if (updates.request.size() == (size_t)central_workers + (size_t)ps::NumGlobalWorkers()) {
        // aggregate gradients and update model
        ApplyUpdates(type, key, &updates, server);
        // notify all workers to call pull
        for (const auto& req : updates.request) {
          server->Response(req, req.sender < ps::kOffset);
        }
        updates.request.clear();
      } else {
        updates.merged.WaitToRead();
      }
    }
  }

  void DataHandleSyncBSCompressed(const DataHandleType type, const ps::KVMeta& req_meta,
                                  const ps::KVPairs<char>& req_data, ps::KVServer<char>* server) {
    // do some check
    CHECK(ps::IsGlobalServer());
    CHECK(req_meta.push);
    CHECK(sync_global_mode_);
    CHECK_EQ(req_data.keys.size(), (size_t)2);
    CHECK_EQ(req_data.lens.size(), (size_t)2);
    CHECK_EQ(req_data.vals.size(), (size_t)req_data.lens[1]);
    int original_size = DecodeKey(req_data.keys[0], true);
    int key = DecodeKey(req_data.keys[1], true);
    auto& stored = has_multi_precision_copy(type) ? store_realt_[key] : store_[key];
    CHECK(!stored.is_none()) << "init " << key << " first";

    size_t ds[] = {(size_t)req_data.lens[1] / mshadow::mshadow_sizeof(type.dtype)};
    TShape dshape(ds, ds + 1);
    TBlob recv_blob;
    MSHADOW_REAL_TYPE_SWITCH(type.dtype, DType, {
      recv_blob = TBlob(reinterpret_cast<DType *>(req_data.vals.data()), dshape, cpu::kDevMask);
    });
    NDArray recved = NDArray(recv_blob, 0);
    dshape = TShape{(int64_t) original_size};

    if (req_meta.sender > ps::kOffset && !ps::EnableCentralWorkers())
      return;

    auto &updates = update_buf_[key];
    if (updates.merged.is_none()) {
      updates.merged = NDArray(dshape, Context(), false,
                               has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype);
    }
    if (updates.temp_array.is_none()) {
      updates.temp_array = NDArray(dshape, Context(), false,
                                   has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype);
    }
    if (updates.request.empty()) {
      gradient_compression_->BSCDecompress(recved, updates.merged, 0);
    } else {
      gradient_compression_->BSCDecompress(recved, updates.temp_array, 0);
      updates.merged += updates.temp_array;
    }
    updates.merged.WaitToRead();
    updates.request.push_back(req_meta);

    int central_workers = 0;
    if (ps::EnableCentralWorkers())  central_workers = ps::NumWorkers();
    if (updates.request.size() == (size_t)central_workers + (size_t)ps::NumGlobalWorkers()) {
      // aggregate gradients and update model
      ApplyUpdates(type, key, &updates, server);
      // notify all workers to call pull
      for (const auto &req : updates.request) {
        server->Response(req, req.sender < ps::kOffset);
      }
      updates.request.clear();
    } else {
      updates.merged.WaitToRead();
    }
  }

  void DataHandleAsyncDefault(const DataHandleType type, const ps::KVMeta& req_meta,
                              const ps::KVPairs<char>& req_data, ps::KVServer<char>* server) {
    // do some check
    CHECK(req_meta.push);
    CHECK_EQ(req_data.keys.size(), (size_t)1);
    CHECK_EQ(req_data.lens.size(), (size_t)1);
    CHECK_EQ(req_data.vals.size(), (size_t)req_data.lens[0]);
    CHECK(ps::IsGlobalServer() && !sync_global_mode_);

    int key = DecodeKey(req_data.keys[0], true);
    auto& stored = has_multi_precision_copy(type) ? store_realt_[key] : store_[key];
    // there used several WaitToRead, this is because \a recved's memory
    // could be deallocated when this function returns. so we need to make sure
    // the operators with \a NDArray are actually finished
    size_t ds[] = {(size_t) req_data.lens[0] / mshadow::mshadow_sizeof(type.dtype)};
    TShape dshape(ds, ds + 1);
    TBlob recv_blob;
    MSHADOW_REAL_TYPE_SWITCH(type.dtype, DType, {
      recv_blob = TBlob(reinterpret_cast<DType*>(req_data.vals.data()), dshape, cpu::kDevMask);
    });
    NDArray recved = NDArray(recv_blob, 0);
    if (stored.is_none()) {
      // initialization
      stored = NDArray(dshape, Context(), false,
                       has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype);
      CopyFromTo(recved, &stored, 0);
      stored.WaitToRead();
      if (has_multi_precision_copy(type)) {
        auto &stored_dtype = store_[key];
        stored_dtype = NDArray(dshape, Context(), false, type.dtype);
        CopyFromTo(stored, stored_dtype);
        stored_dtype.WaitToRead();
      }
      server->Response(req_meta);
      mu_.lock();
      initialized_[key] = true;
      mu_.unlock();
    } else {
      // ignore central workers if DMLC_ENABLE_CENTRAL_WORKER is not set
      if (req_meta.sender > ps::kOffset && !ps::EnableCentralWorkers()) return;

      auto &updates = update_buf_[key];
      if (updates.merged.is_none()) {
        updates.merged = NDArray(dshape, Context(), false,
                                 has_multi_precision_copy(type) ? mshadow::kFloat32 : type.dtype);
      }
      if (has_multi_precision_copy(type) && updates.temp_array.is_none()) {
        updates.temp_array = NDArray(dshape, Context(), false, mshadow::kFloat32);
      }

      if (req_meta.sender > ps::kOffset) {
        // push from central workers, aggregate
        if (updates.request.empty()) {
          CopyFromTo(recved, updates.merged);
        } else {
          if (has_multi_precision_copy(type)) {
            CopyFromTo(recved, updates.temp_array);
            updates.merged += updates.temp_array;
          } else {
            updates.merged += recved;
          }
        }
        updates.merged.WaitToRead();
      } else {
        // push from servers
        if (has_multi_precision_copy(type)) {
          CopyFromTo(recved, updates.temp_array);
        } else {
          updates.temp_array = recved;
        }
        updates.temp_array.WaitToRead();
      }

      if (req_meta.sender > ps::kOffset) {
        // push from worker
        updates.request.push_back(req_meta);
        if (updates.request.size() == (size_t)ps::NumWorkers()) {
          ApplyUpdates(type, key, true, &updates, server);
          for (const auto& req : updates.request) {
            CHECK(req.sender > ps::kOffset);
            server->Response(req, false);
          }
          updates.request.clear();
        } else {
          updates.merged.WaitToRead();
        }
      } else {
        // push from server
        ApplyUpdates(type, key, false, &updates, server);
        server->Response(req_meta, true);
      }
    }
  }

  void DataHandleAsyncCompressed(const DataHandleType type, const ps::KVMeta& req_meta,
                                 const ps::KVPairs<char>& req_data, ps::KVServer<char>* server) {
    // do some check
    CHECK(req_meta.push);
    CHECK_EQ(req_data.keys.size(), (size_t)2);
    CHECK_EQ(req_data.lens.size(), (size_t)2);
    CHECK_EQ(req_data.vals.size(), (size_t)req_data.lens[1]);
    CHECK(ps::IsGlobalServer() && !sync_global_mode_);
    CHECK_EQ(type.dtype, mshadow::kFloat32)
            << "Gradient compression is currently supported for fp32 only";

    int original_size = DecodeKey(req_data.keys[0], true);
    int key = DecodeKey(req_data.keys[1], true);
    auto& stored = store_[key];
    // there used several WaitToRead, this is because \a recved's memory
    // could be deallocated when this function returns. so we need to make sure
    // the operators with \a NDArray are actually finished
    size_t ds[] = {(size_t)req_data.lens[1] / mshadow::mshadow_sizeof(type.dtype)};
    TShape dshape(ds, ds + 1);
    TBlob recv_blob(reinterpret_cast<real_t*>(req_data.vals.data()), dshape, cpu::kDevMask);
    NDArray recved = NDArray(recv_blob, 0);

    NDArray decomp_buf = decomp_buf_[key];
    dshape = TShape{(int64_t) original_size};
    if (decomp_buf.is_none()) {
      decomp_buf = NDArray(dshape, Context());
    }

    if (stored.is_none()) {
      // initialization
      stored = NDArray(dshape, Context());
      gradient_compression_->Dequantize(recved, &stored, 0);
      server->Response(req_meta);
      stored.WaitToRead();
      if (ps::IsGlobalServer()) {
        mu_.lock();
        initialized_[key] = true;
        mu_.unlock();
      }
    } else {
      // ignore central workers if DMLC_ENABLE_CENTRAL_WORKER is not set
      if (req_meta.sender > ps::kOffset && !ps::EnableCentralWorkers()) return;

      auto& updates = update_buf_[key];
      if (updates.merged.is_none()) {
        updates.merged = NDArray(dshape, Context());
      }

      if (req_meta.sender > ps::kOffset) {
        // push from central workers, aggregate
        if (updates.request.empty()) {
          gradient_compression_->Dequantize(recved, &updates.merged, 0);
        } else {
          gradient_compression_->Dequantize(recved, &decomp_buf, 0);
          updates.merged += decomp_buf;
        }
        updates.merged.WaitToRead();
      } else {
        // push from servers
        gradient_compression_->Dequantize(recved, &decomp_buf, 0);
      }

      if (req_meta.sender > ps::kOffset) {
        // push from central workers
        updates.request.push_back(req_meta);
        if (updates.request.size() == (size_t)ps::NumWorkers()) {
          ApplyUpdates(type, key, true, &updates, server);
          for (const auto& req : updates.request) {
            CHECK(req.sender > ps::kOffset);
            server->Response(req, false);
          }
          updates.request.clear();
        } else {
          updates.merged.WaitToRead();
        }
      } else {
        // push from servers
        CHECK(updater_);
        exec_.Exec([this, key, &decomp_buf, &stored]() {
            updater_(key, decomp_buf, &stored);
        });
        server->Response(req_meta, true);
        stored.WaitToRead();
      }
    }
  }

  void DataHandleAsyncBSCompressed(const DataHandleType type, const ps::KVMeta& req_meta,
                                   const ps::KVPairs<char>& req_data, ps::KVServer<char>* server) {
    // todo.
  }

  void DataHandlePullDefault(const DataHandleType type, const ps::KVMeta& req_meta,
                             const ps::KVPairs<char>& req_data, ps::KVServer<char>* server) {
    // do some check
    CHECK(!req_meta.push);
    CHECK_EQ(req_data.keys.size(), (size_t)1);

    bool request = req_meta.sender % 2 == 1;
    if (req_meta.sender > 100) {
      if (request) {
        // response pull requests
        int key = ps_server_->enable_p3 ? (int)req_data.keys[0]:
          DecodeKey(req_data.keys[0], ps::IsGlobalServer());

        mu_.lock();
        auto& init = initialized_[key];
        mu_.unlock();
        while (!init.load()) {
          std::this_thread::sleep_for(std::chrono::milliseconds(100));
        }
        DefaultStorageResponse(type, key, req_meta, req_data, server);
      } else {
        // receive and handle pull data
        DataHandlePullResponseDefault(type, req_meta, req_data, server);
      }
    } else {
      if (ps_server_->enable_inter_ts) {
        if (request && ps::IsGlobalServer()) {
          // response pull requests
          int key = DecodeKey(req_data.keys[0], ps::IsGlobalServer());
          mu_.lock();
          auto &init = initialized_[key];
          mu_.unlock();
          while (!init.load()) {
            std::this_thread::sleep_for(std::chrono::milliseconds(100));
          }
          DefaultStorageResponse(type, key, req_meta, req_data, server);
        } else {
          // receive and handle pull data
          DataHandlePullResponseDefault(type, req_meta, req_data, server);
        }
      } else {
        if (request) {
          // response pull requests
          int key = ps_server_->enable_p3 ? (int)req_data.keys[0]:
            DecodeKey(req_data.keys[0], ps::IsGlobalServer());
          mu_.lock();
          auto &init = initialized_[key];
          mu_.unlock();
          while (!init.load()) {
            std::this_thread::sleep_for(std::chrono::milliseconds(100));
          }
          DefaultStorageResponse(type, key, req_meta, req_data, server);
        } else {
          // receive and handle pull data
          DataHandlePullResponseDefault(type, req_meta, req_data, server);
        }
      }
    }
  }

  int DecodeKey(ps::Key key, bool is_global=false) {
    auto kr = ps::Postoffice::Get()->GetServerKeyRanges(is_global)[ps::MyRank(is_global)];
    return key - kr.begin();
  }

  PSKV& EncodeDefaultKey(const int key, const size_t num_arr_elems, const int num_bytes) {
    mu_.lock();
    PSKV& pskv = ps_kv_[key];
    mu_.unlock();
    size_t pskv_size = num_arr_elems * num_bytes;
    if (!pskv.keys.empty()) {
      CHECK_EQ(static_cast<size_t>(pskv.size), pskv_size)
        << "The value size cannot be changed " << pskv_size << ". Key is " << key;
    } else {
      auto krs = ps::Postoffice::Get()->GetServerKeyRanges(true);
      const int num_global_servers = krs.size();
      CHECK_GT(num_global_servers, 0);
      // a simple heuristic for load balance
      if (num_arr_elems < bigarray_bound_) {
        // send it to a single random picked global server
        int global_server = (key * 9973) % num_global_servers;
        ps::Key ps_key = krs[global_server].begin() + key;
        CHECK_LT(ps_key, krs[global_server].end());
        pskv.keys.push_back(ps_key);
        const int total_bytes = num_arr_elems * num_bytes;
        pskv.lens.push_back(total_bytes);
        pskv.size = total_bytes;
      } else {
        // parition it to all servers
        pskv.size = 0;
        for (int i = 0; i < num_global_servers; ++i) {
          size_t part_size =
            static_cast<size_t>(round(static_cast<double>(num_arr_elems)/num_global_servers*(i+1))) -
            static_cast<size_t>(round(static_cast<double>(num_arr_elems)/num_global_servers*i));
          ps::Key ps_key = krs[i].begin() + key;
          CHECK_LT(ps_key, krs[i].end());
          pskv.keys.push_back(ps_key);
          const int total_bytes = part_size * num_bytes;
          pskv.lens.push_back(total_bytes);
          pskv.size += total_bytes;
        }
      }
      CHECK_EQ(static_cast<size_t>(pskv.size), pskv_size);
    }
    return pskv;
  }

  PSKV& EncodeCompressedKey(const int key, const size_t original_num_elem,
                            const bool is_push, const int num_bytes) {
    auto krs = ps::Postoffice::Get()->GetServerKeyRanges(true);
    const int num_global_servers = krs.size();
    CHECK_GT(num_global_servers, 0);

    // represents size of data to be sent
    size_t compr_num_elem;
    if (gradient_compression_->get_type() == CompressionType::kTwoBit) {
      compr_num_elem = gradient_compression_->GetCompressedSize(original_num_elem);
    }
    else if (gradient_compression_->get_type() == CompressionType::kBiSparseCompression) {
      compr_num_elem = original_num_elem * gradient_compression_->get_threshold() * 2;
    }
    mu_.lock();
    PSKV& pskv = is_push ? compr_ps_kv_[key].push : compr_ps_kv_[key].pull;
    mu_.unlock();

    if (!pskv.keys.empty()) {
      const size_t num_elem = is_push ? compr_num_elem : original_num_elem;
      CHECK_EQ(static_cast<size_t>(pskv.size), num_elem * num_bytes)
        << "The value size can't be changed. For key " << key;
    } else {
      // populate both pull and push pskvs
      // push pskv has sizes corresponding to compressed data
      // pull pskv has decompressed sizes for parts in push_pskv
      mu_.lock();
      PSKV& pull_pskv = compr_ps_kv_[key].pull;
      PSKV& push_pskv = compr_ps_kv_[key].push;
      mu_.unlock();

      if (original_num_elem < bigarray_bound_) {
        // send it to a single random picked server
        int global_server = (key * 9973) % num_global_servers;
        ps::Key ps_key = krs[global_server].begin() + key;
        CHECK_LT(ps_key, krs[global_server].end());
        // meta info
        push_pskv.keys.push_back(krs[global_server].begin() + original_num_elem);
        push_pskv.lens.push_back(0);
        // data
        push_pskv.keys.push_back(ps_key);
        pull_pskv.keys.push_back(ps_key);
        const int compr_size = compr_num_elem * num_bytes;
        const int original_size = original_num_elem * num_bytes;
        push_pskv.lens.push_back(compr_size);
        pull_pskv.lens.push_back(original_size);
        push_pskv.size = compr_size;
        pull_pskv.size = original_size;
      } else {
        // partition it to all servers
        push_pskv.size = 0;
        pull_pskv.size = 0;
        for (int i = 0; i < num_global_servers; ++i) {
          size_t part_compr, part_orig;
          if (i == num_global_servers - 1) {
            part_compr = compr_num_elem - push_pskv.size;
            part_orig = original_num_elem - pull_pskv.size;
          } else {
            part_compr =
              static_cast<size_t>(round(static_cast<double>(compr_num_elem)/num_global_servers*(i+1))) -
              static_cast<size_t>(round(static_cast<double>(compr_num_elem)/num_global_servers*i));
            part_orig = part_compr * gradient_compression_->GetCompressionFactor();
          }
          // meta info
          ps::Key ps_key_dummy = krs[i].begin() + part_orig;
          CHECK_LT(ps_key_dummy, krs[i].end());
          push_pskv.keys.push_back(ps_key_dummy);
          push_pskv.lens.push_back(0);
          // data
          ps::Key ps_key = krs[i].begin() + key;
          CHECK_LT(ps_key, krs[i].end());
          push_pskv.keys.push_back(ps_key);
          pull_pskv.keys.push_back(ps_key);
          push_pskv.lens.push_back(part_compr * num_bytes);
          pull_pskv.lens.push_back(part_orig * num_bytes);
          // num elements need to be inserted below so that for last server,
          // there is no round off error
          push_pskv.size += part_compr;
          pull_pskv.size += part_orig;
        }
        CHECK_EQ(static_cast<size_t>(push_pskv.size), compr_num_elem);
        CHECK_EQ(static_cast<size_t>(pull_pskv.size), original_num_elem);
        push_pskv.size *= num_bytes;
        pull_pskv.size *= num_bytes;
        CHECK_EQ(push_pskv.lens.size(), num_global_servers * 2);
      }
    }
    return pskv;
  }

  PSKV& EncodeBSCompressedKey(const int key, const size_t original_num_elem,
                              const bool is_push, const int num_bytes) {
    auto krs = ps::Postoffice::Get()->GetServerKeyRanges(true);
    const int num_global_servers = krs.size();
    CHECK_GT(num_global_servers, 0);

    // represents size of data to be sent
    size_t compr_num_elem;
    size_t compr_pull_num_elem;
      CHECK(gradient_compression_->get_type() == CompressionType::kBiSparseCompression)
              << "EncodeBSCompressedKey misused";
    compr_num_elem = original_num_elem * gradient_compression_->get_threshold() * 2;
    compr_pull_num_elem = compr_num_elem * ps::NumGlobalWorkers();
    mu_.lock();
    PSKV& pskv = is_push ? compr_ps_kv_[key].push : compr_ps_kv_[key].pull;
    mu_.unlock();

    if (!pskv.keys.empty()) {
      const size_t num_elem = is_push ? compr_num_elem : compr_pull_num_elem;
      CHECK_EQ(static_cast<size_t>(pskv.size), num_elem * num_bytes)
        << "The value size can't be changed. For key " << key;
    } else {
      // populate both pull and push pskvs
      // push pskv has sizes corresponding to compressed data
      // pull pskv has decompressed sizes for parts in push_pskv
      mu_.lock();
      PSKV& pull_pskv = compr_ps_kv_[key].pull;
      PSKV& push_pskv = compr_ps_kv_[key].push;
      mu_.unlock();

      if (true) {
        // send it to a single random picked server
        int global_server = (key * 9973) % num_global_servers;
        ps::Key ps_key = krs[global_server].begin() + key;
        CHECK_LT(ps_key, krs[global_server].end());
        // meta info
        push_pskv.keys.push_back(krs[global_server].begin() + original_num_elem);
        push_pskv.lens.push_back(0);
        // data
        push_pskv.keys.push_back(ps_key);
        pull_pskv.keys.push_back(ps_key);
        const int compr_size = compr_num_elem * num_bytes;
        const int compr_pull_size = compr_pull_num_elem * num_bytes;
        push_pskv.lens.push_back(compr_size);
        pull_pskv.lens.push_back(compr_pull_size);
        push_pskv.size = compr_size;
        pull_pskv.size = compr_pull_size;
      } else {
        // no support for BSC now
        // partition it to all servers
        push_pskv.size = 0;
        pull_pskv.size = 0;
        for (int i = 0; i < num_global_servers; ++i) {
          size_t part_compr, part_orig;
          if (i == num_global_servers - 1) {
            part_compr = compr_num_elem - push_pskv.size;
            part_orig = original_num_elem - pull_pskv.size;
          } else {
            part_compr =
              static_cast<size_t>(round(static_cast<double>(compr_num_elem)/num_global_servers*(i+1))) -
              static_cast<size_t>(round(static_cast<double>(compr_num_elem)/num_global_servers*i));
              part_orig = part_compr * ps::NumGlobalWorkers();
          }
          // meta info
          ps::Key ps_key_dummy = krs[i].begin() + part_orig;
          CHECK_LT(ps_key_dummy, krs[i].end());
          push_pskv.keys.push_back(ps_key_dummy);
          push_pskv.lens.push_back(0);
          // data
          ps::Key ps_key = krs[i].begin() + key;
          CHECK_LT(ps_key, krs[i].end());
          push_pskv.keys.push_back(ps_key);
          pull_pskv.keys.push_back(ps_key);
          push_pskv.lens.push_back(part_compr * num_bytes);
          pull_pskv.lens.push_back(part_orig * num_bytes);
          // num elements need to be inserted below so that for last server,
          // there is no round off error
          push_pskv.size += part_compr;
          pull_pskv.size += part_orig;
        }
        CHECK_EQ(static_cast<size_t>(push_pskv.size), compr_num_elem);
        CHECK_EQ(static_cast<size_t>(pull_pskv.size), original_num_elem);
        push_pskv.size *= num_bytes;
        pull_pskv.size *= num_bytes;
        CHECK_EQ(push_pskv.lens.size(), num_global_servers * 2);
      }
    }
    return pskv;
  }

  std::unordered_map<int, PSKV> ps_kv_;
  std::unordered_map<int, ComprPSKV> compr_ps_kv_;
  size_t bigarray_bound_;
  size_t size_lower_bound;
  bool use_hfa;
  size_t local_iters;
  size_t period_k1;
  size_t period_k2;
  std::mutex mu_;

  bool sync_mode_ = false;
  bool sync_global_mode_ = false;
  KVStore::Controller controller_;
  KVStore::Updater updater_;

  std::unordered_map<int, NDArray> store_;
  std::unordered_map<int, NDArray> store_milestone_;
  std::unordered_map<int, NDArray> store_realt_;
  std::unordered_map<int, int> store_v_;
  std::mutex store_mu_;
  std::unordered_map<int, NDArray> comm_buf_;

  /**
   * \brief merge_buf_ is a buffer used if sync_mode is true. It represents
   * values from different workers being merged. The store will be updated
   * to this value when values from all workers are pushed into this buffer.
   */
  std::unordered_map<int, UpdateBuf> update_buf_;
  std::unordered_map<int, UpdateBuf> update_buf_tmp_;

  /**
   * \brief decomp_buf_ is a buffer into which compressed values are
   * decompressed before merging to the store. used when compress_!='none'
   */
  std::unordered_map<int, NDArray> decomp_buf_;

  /**
   * \brief buffer for compressed data
   * Used when gradient compression is active and action is push
   */
  std::unordered_map<int, NDArray> compr_buf_;
  std::unordered_map<int, NDArray> velocity_;
  std::unordered_map<int, NDArray> accumulated_velocity_;

  /**
   * \brief residual buffer to accumulate quantization error
   * during gradient compression
   */
  std::unordered_map<int, NDArray> residual_;

  Executor exec_;
  int num_stop_executor_ = 0;

  ps::KVServer<char>* ps_server_;

  // whether to LOG verbose information
  bool log_verbose_;

  // whether the server is initialized and ready to response pull request.
  std::unordered_map<int, std::atomic<bool>> initialized_;

  // data buffer for received kvs for each key
  std::unordered_map<int, std::vector<ps::KVPairs<char>>> recv_kvs_;

  // map ps_keys to keys
  std::unordered_map<ps::Key, int> key_map_;

  // map timestamp of push requests to keys
  std::unordered_map<int, int> ts_key_map_;

  /*
   * \brief whether to use multi precision mode.
   * in multi precision mode, all weights are stored as float32.
   * any gradient received will be cast to float32 before accumulation and updating of weights.
   */
  bool multi_precision_;

  /**
   * \brief gradient compression object.
   * starts with none, used after SetGradientCompression sets the type
   * currently there is no support for unsetting gradient compression
   */
  std::shared_ptr<kvstore::GradientCompression> gradient_compression_;
};
}  // namespace kvstore
}  // namespace mxnet
#endif  // MXNET_KVSTORE_KVSTORE_DIST_SERVER_H_