# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Zonghang Li
# This file is distributed under the same license as the GeoMX package.
# Zonghang Li <lizhuestc@gmail.com>, 2023.

msgid ""
msgstr ""
"Project-Id-Version: GeoMX 1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-02 09:50+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Li, Zonghang <lizhuestc@gmail.com>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <lizhuestc@gmail.com>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/accelerator.rst:2 1ce840bf6efa44ca9677d10ecc9b4e2a
msgid "How to Use GeoMX Accelerators?"
msgstr ""

#: ../../source/accelerator.rst:4 c97753cff827411e9508a8de7397f53d
msgid ""
"Given the often limited and varied network conditions in WANs, "
"distributed training across data centers can potentially create "
"communication bottlenecks. To mitigate these issues, GeoMX employs a "
"variety of optimization techniques. These include gradient "
"sparsification, mixed-precision quantization, advanced transmission "
"protocols, synchronization algorithms, flow scheduling, and priority "
"scheduling, among others (e.g., overlay scheduling, currently in "
"development). These techniques comprehensively tackle communication "
"issues, further enhancing the efficiency and robustness of distributed "
"machine learning training in GeoMX."
msgstr ""

#: ../../source/accelerator.rst:15 dfa4728e6b234ca39ee0cc362b3c3e2f
msgid ""
"This guidance describes the environmental variables and hyperparameters "
"needed to launch each optimization technique in our GeoMX system."
msgstr ""

#: ../../source/accelerator.rst:21 1f8d219bf1ce436fa22cd964b1971974
msgid "Bidirectional Gradient Sparsification"
msgstr ""

#: ../../source/accelerator.rst:23 85a2b96af1854875a4bb79e271a47741
msgid ""
"Traditional approaches such as `Deep Gradient Compression "
"<https://arxiv.org/pdf/1712.01887.pdf>`__ sparsify the pushed gradient "
"tensors. For further compression, we also sparsify the pulled "
"(aggregated) gradient tensors rather than pulling full parameters. This "
"technique is enabled between the global parameter server and the intra-"
"domain parameter servers of different data centers. (Refer to `this paper"
" <https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-"
"cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`__"
" for more details.)"
msgstr ""

#: ../../source/accelerator.rst:33 8f0966f16ff54460a7f29e9d371cbedd
msgid ""
"To enable bidirectional gradient sparsification, define it in "
"``kvstore_dist.set_gradient_compression`` and set the compression ratio:"
msgstr ""

#: ../../source/accelerator.rst:69 b46ea807915549d4a25e7ab7811f930d
msgid ""
"Note that gradient tensors are classified into large and tiny tensors "
"based on their size, and only the large tensors will be sparsified for "
"transmission. The threshold for classifying large and tiny tensors can be"
" set through the environmental variable "
"``MXNET_KVSTORE_SIZE_LOWER_BOUND``. For example:"
msgstr ""

#: ../../source/accelerator.rst:79 329bc67ebe074da0ad7ea0d63b8aad81
msgid ""
"The demo code can be found in `examples/cnn_bsc.py <https://github.com"
"/INET-RC/GeoMX/blob/main/examples/cnn_bsc.py>`_. You can run this demo by"
" simply ``bash scripts/xpu/run_bisparse_compression.sh``, where ``xpu`` "
"should be ``cpu`` or ``gpu``."
msgstr ""

#: ../../source/accelerator.rst:86 29da9417ae3c496b840a6f3acacc3e06
msgid "Low-Precision Quantization"
msgstr ""

#: ../../source/accelerator.rst:88 0ca9d9c212a547d4ba6c849a78e6e593
msgid ""
"GeoMX also supports quantifying model data at lower precision for "
"transmission, such as in FP16 format. In this scheme, GeoMX computes the "
"model using FP32, but during transmission, it converts the model data "
"tensor into FP16. Once the pulling data is received, GeoMX reverts it "
"back into FP32 and continues model computing. This effectively halves the"
" data traffic volume over both LANs and WANs."
msgstr ""

#: ../../source/accelerator.rst:95 c67ef41d62cf4dfab021e8ce95ea0310
msgid ""
"To quantify model data for transmission in FP16 format, we can simply "
"convert the numerical precision of tensors in our Python code using "
"``astype('float16')``:"
msgstr ""

#: ../../source/accelerator.rst:134 aeada72c8e6143719f0712a95e95906e
msgid ""
"The demo code is provided in `examples/cnn_fp16.py <https://github.com"
"/INET-RC/GeoMX/blob/main/examples/cnn_fp16.py>`_, we can run it using "
"``bash scripts/xpu/run_fp16.sh``, where ``xpu`` should be ``cpu`` or "
"``gpu``."
msgstr ""

#: ../../source/accelerator.rst:142 c546dfa967a04b1fb04c080c66f47857
msgid "Mixed-Precision Quantization"
msgstr ""

#: ../../source/accelerator.rst:144 7d2662abce5a4f4cac25b6084a6aa5d9
msgid ""
"The technology of Mixed-Precision Quantization (MPQ) leverages both Bi-"
"Sparse and FP16. In this scheme, tiny tensors are quantified into FP16 "
"format for transmission, while large tensors persist in the FP32 format. "
"Moreover, these large sensors will undergo a sparsification process "
"before transmission. This precaution is taken to minimize the loss of "
"crucial information and avoid significant degradation to model "
"performance."
msgstr ""

#: ../../source/accelerator.rst:152 3d666e8325d848d799978f076f188f18
msgid "Table 1: Summary of the application scope for Bi-Sparse, FP16, and MPQ."
msgstr ""

#: ../../source/accelerator.rst:158 3c548b8bace94594835e132639bc09af
msgid "Intra-Data Center"
msgstr ""

#: ../../source/accelerator.rst:160 c843c8619376427aa62f6cc010495cdb
msgid "Inter-Data Centers"
msgstr ""

#: ../../source/accelerator.rst:163 ../../source/accelerator.rst:165
#: b3333b6e8edf411d91d21ed074572fa7 c3ede880c3a244fb9bbafbe86024956d
msgid "Large Tensors"
msgstr ""

#: ../../source/accelerator.rst:164 ../../source/accelerator.rst:166
#: 6122b5efe0d347cb91aeb917890815b9 dee77ed9c22e4473b558e9674e697ddf
msgid "Tiny Tensors"
msgstr ""

#: ../../source/accelerator.rst:167 350a4cfdf512475099498a80dcf488a8
msgid "Bi-Sparse"
msgstr ""

#: ../../source/accelerator.rst:168 ../../source/accelerator.rst:169
#: ../../source/accelerator.rst:171 ../../source/accelerator.rst:178
#: 15d6179dd1cb49a8bd0cdb68fda86c79 53a41f5af0fb4c57ada2848a40c6f1cb
#: 5e9a55bb0acc4591b6e447f07564188e 9200046f8f0846da835cc43473158883
msgid "FP32, Dense"
msgstr ""

#: ../../source/accelerator.rst:170 ../../source/accelerator.rst:180
#: 367a45ae031a4da4a06a20e666e68b1f bf19a8a56c3d4c868d2867a0efee8aaf
msgid "FP32, Sparse"
msgstr ""

#: ../../source/accelerator.rst:172 a821c0cc4b204540b3c4d77966077b78
msgid "FP16"
msgstr ""

#: ../../source/accelerator.rst:173 ../../source/accelerator.rst:174
#: ../../source/accelerator.rst:175 ../../source/accelerator.rst:176
#: ../../source/accelerator.rst:179 ../../source/accelerator.rst:181
#: 0678a112658c4cab84bc01916bb8696f 3473b0cda3f54311b9c5b40143b56dcd
#: 4f2817cdbfc742658921b58bf216c146 7b3e4be9784f43328fab6a1659c21cc5
#: a31c33c3aa6b4945ac60651d2a39eca7 c665d97dd71945cb85b1472b74730f85
msgid "FP16, Dense"
msgstr ""

#: ../../source/accelerator.rst:177 e9034b1dab59475fbec74ea8259d425f
msgid "MPQ"
msgstr ""

#: ../../source/accelerator.rst:184 44acfce73d6e4e29b9edc72f7044a6b6
msgid ""
"For details on how to classify large and tiny tensors, please refer to "
"the :ref:`bidirectional-gradient-sparsification` section. The demo code "
"for using MPQ is given below:"
msgstr ""

#: ../../source/accelerator.rst:235 c8543b5b8aa5480ba1115583d85e3a0e
msgid ""
"You can also find them in `examples/cnn_mpq.py <https://github.com/INET-"
"RC/GeoMX/blob/main/examples/cnn_mpq.py>`_ and run this demo by executing "
"``scripts/xpu/run_mixed_precision.sh``, where ``xpu`` should be ``cpu`` "
"or ``gpu``."
msgstr ""

#: ../../source/accelerator.rst:243 00d807fc4d2b4a07ac74bb37c853cc7d
msgid "Differential Gradient Transmission"
msgstr ""

#: ../../source/accelerator.rst:245 9d0b645b964d430188b11819462d0ca8
msgid ""
"Differential Gradient Transmission (DGT) is an optimized transmission "
"protocol for distributed machine learning tasks. Leveraging the tolerance"
" of gradient descent algorithms towards partial parameter loss, this "
"protocol transfers gradients across multiple channels, each with distinct"
" levels of reliability and priority, contingent on their respective "
"contributions to model convergence. Through these prioritized channels, "
"critical gradients receive precedence in transmission, while other non-"
"important gradients are transmitted with lower priority and reliability. "
"This helps to reduce tail latency and thus reduce the end-to-end "
"transmission delay of parameter synchronization. (Refer to `this paper "
"<https://drive.google.com/file/d/1IbmpFybX_qXZM2g_8BrcD9IF080qci94/view>`__"
" for more details and `this repo <https://github.com/zhouhuaman/dgt>`__ "
"for individual use.)"
msgstr ""

#: ../../source/accelerator.rst:260 5dcb0ccd73ef428e953f1aaa8cb34ea7
msgid "To enable DGT, set the following environment variables:"
msgstr ""

#: ../../source/accelerator.rst:269 822d7d950a444ae6af21be05164f4aa2
msgid "Use the demo script ``scripts/xpu/run_dgt.sh`` to try it!"
msgstr ""

#: ../../source/accelerator.rst:274 d77a713fd2e84747baf90412ebc7f80e
msgid "TSEngine"
msgstr ""

#: ../../source/accelerator.rst:276 094269145c9145fb940f4415776b3731
msgid ""
"To solve the communication in-cast issue typically associated with "
"centralized parameter servers, GeoMX incorporates TSEngine, an adaptive "
"communication scheduler designed for efficient communication overlay in "
"WANs. TSEngine dynamically optimizes the topology overlay and "
"communication logic among the training nodes in response to real-time "
"network conditions. This adaptive scheduler shows significant advantages "
"over existing communication patterns in terms of system efficiency, "
"communication, as well as scalability. (Refer to `this paper "
"<https://drive.google.com/file/d/1ELfApVoCA8WCdOe3iBe-"
"VreLJCSD7r8r/view>`__ for more details and `this repo "
"<https://github.com/zhouhuaman/TSEngine>`__ for individual use.)"
msgstr ""

#: ../../source/accelerator.rst:288 835e8ab0ceb0414282d1beaf054bb7f1
msgid ""
"Similar to DGT, only a few environment variables are required to enable "
"TSEngine:"
msgstr ""

#: ../../source/accelerator.rst:297 b2a433eb13b0487c86df8bfd875f07cb
msgid "Use the demo script ``scripts/xpu/run_tsengine.sh`` to try it!"
msgstr ""

#: ../../source/accelerator.rst:300 7ae7b496ffc244e6854101fbd666710a
msgid ""
"If ``ENABLE_INTER_TS`` is used, then TSEngine is enabled across data "
"centers. Instead, if ``ENABLE_INTRA_TS`` is used, then TSEngine is "
"enabled inside the data center. In this example, both ``ENABLE_INTER_TS``"
" and ``ENABLE_INTRA_TS`` are enabled, but we can also choose to enable "
"only one."
msgstr ""

#: ../../source/accelerator.rst:309 48dc1297ee814bf0905d2bac8f013aa3
msgid "Priority-based Parameter Propagation"
msgstr ""

#: ../../source/accelerator.rst:311 792beeb9c82143b184c97c712a1778e7
msgid ""
"In conventional implementations, the gradient synchronization at round "
":math:`r` does not overlap with the forward propagation at round "
":math:`r+1`, because the forward propagation relies on the completion of "
"gradient synchronization. To improve system efficiency, GeoMX integrates "
"the Priority-based Parameter Propagation (P3) scheduler, which "
"prioritizes the transmission of shallow-layer gradients. This setup "
"enables overlapping between forward propagation and gradient "
"synchronization, allowing earlier execution of forward propagation for "
"the next round, thereby accelerating distributed training. (See `this "
"paper <https://arxiv.org/pdf/1905.03960.pdf>`__ for more details and "
"`this repo <https://github.com/anandj91/p3>`__ for individual use.)"
msgstr ""

#: ../../source/accelerator.rst:323 882bcaf82c0e4f2dbd21889a4016aaa0
msgid "To enable P3, only one environment variable is required:"
msgstr ""

#: ../../source/accelerator.rst:329 9ce2dc6c441a487bae69e5adec966a6e
msgid "Use the demo script ``scripts/xpu/run_p3.sh`` to try it!"
msgstr ""

#: ../../source/accelerator.rst:332 334605aaeb8644febe89d037969d1053
msgid "Multi-Server Load Balancing"
msgstr ""

#: ../../source/accelerator.rst:334 d43ec28e447f4dd1a1df9bb35b7effb1
msgid ""
"GeoMX supports a balanced distribution of workload, including traffic, "
"storage, and computation, across multiple global parameter servers. By "
"preventing any single server from becoming a bottleneck, Multi-Server "
"Load Balancing (MultiGPS) significantly enhances efficiency, scalability,"
" and overall performance of our GeoMX system."
msgstr ""

#: ../../source/accelerator.rst:340 a3bc260bdeae412e996b681fac675dee
msgid ""
"To enable MultiGPS, set ``DMLC_NUM_GLOBAL_SERVER`` and some "
"``DMLC_NUM_SERVER`` to an integer greater than 1."
msgstr ""

#: ../../source/accelerator.rst:363 a02c69afbafc4573b234a357cb666ff9
msgid "Use the demo script ``scripts/xpu/run_multi_gps.sh`` to try it!"
msgstr ""

