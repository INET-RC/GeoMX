# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Zonghang Li
# This file is distributed under the same license as the GeoMX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.

msgid ""
msgstr ""
"Project-Id-Version: GeoMX 1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-02 09:50+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Li, Zonghang <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <lizhuestc@gmail.com>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/index.rst:11
msgid "GeoMX Tutorials"
msgstr "GeoMX 教程"

#: ../../source/index.rst:7 ee1a1e4273a34dcea8ee2b229efe3e28
msgid "Welcome to GeoMX Docs! 😁"
msgstr "欢迎来到 GeoMX 的中文文档！😁"

#: ../../source/index.rst:26 32bf00cb9bc74913bbf629a4d0ec3647
msgid ""
"**GeoMX** is a fast and unified distributed system for training ML "
"algorithms over geographical data centers. Built upon the `MXNET "
"<https://github.com/apache/mxnet>`_ framework, GeoMX integrates several "
"sophisticated optimization techniques to enhance its training efficiency."
" These strategic enhancements result in a significant performance boost "
"compared to the original MXNET system, offering 20x acceleration under "
"identical network bandwidth conditions. This superior efficiency propels "
"GeoMX to the forefront of training systems used for geographically "
"dispersed data centers, showcasing satisfying performance and "
"effectiveness."
msgstr "GeoMX 是一个高性能分布式机器学习系统，它用于在地理分散的多个数据中心之间"
"进行协同的机器学习模型训练。GeoMX 是基于 `MXNET <https://github.com/apache/mxnet>`_ "
"系统建立的，并且融合了一系列复杂的优化技术以提高训练效率。通过这些改进，相比于原始的 MXNET "
"系统，在相同网络带宽的条件下，GeoMX 的训练速度提升了 20 倍。这种显著的效率提升使得 GeoMX "
"适用于地理分散的数据中心场景，在跨广域的分布式机器学习应用中表现出了优越的性能和效率。"

#: ../../source/index.rst:28 284dc6e7dd40416b8252f441ed8a2f57
msgid ""
"GeoMX employs the `Hierarchical Parameter Server (HiPS) "
"<https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-"
"cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_"
" framework as its fundamental training architecture, designed to "
"segregate the network environment within and beyond the data center. This"
" unique architecture includes an intra-domain parameter server system "
"within each data center while concurrently establishing an inter-"
"parameter server connection between different data centers. In this "
"configuration, model data traffic undergoes two stages of aggregation: an"
" initial local aggregation within each data center, followed by a global "
"aggregation at the central data center. This approach effectively "
"minimizes cross-WAN traffic and, consequently, reduces communication "
"overhead."
msgstr "GeoMX 采用一种名为分层参数服务器 (`HiPS <https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_) "
"的新型架构作为它的基础训练架构。HiPS 架构分隔了数据中心内外部网络环境，它在每个数据"
"中心内部建立域内参数服务器系统，同时在不同的数据中心之间也建立起了全局参数服务器系统。"
"在这种设置下，模型数据的传输会经历两个阶段的聚合：首先，在各自的数据中心内部局部聚合，"
"然后，在全局数据中心进行全局聚合。这种方法有效减小了跨广域网的通信流量，从而显著降低通信开销。"

#: ../../source/index.rst:30 0b759d2159d244808f1b652d2b99f79c
msgid ""
"But it's far from enough, given the often limited and varied network "
"conditions in WANs, distributed training across data centers can "
"potentially create communication bottlenecks. To mitigate these issues, "
"GeoMX employs a variety of optimization techniques. These include "
"gradient sparsification, low-precision quantization (e.g., fp16), mixed-"
"precision quantization, advanced transmission protocols, synchronization "
"algorithms, flow scheduling, and priority scheduling, among others (e.g.,"
" overlay scheduling, currently in development). These techniques "
"comprehensively tackle communication issues, further enhancing the "
"efficiency and robustness of distributed machine learning training in "
"GeoMX."
msgstr "但是，仅有这些还远远不够。考虑到广域网中的网络资源常常是受限的，并且随时间"
"动态时变，跨数据中心的分布式训练仍然面临诸多通信瓶颈。为了缓解这些瓶颈，GeoMX 采用"
"了多种优化技术。这些技术包括梯度稀疏化、低精度量化（例如 FP16）、混合精度量化、更先"
"进的传输协议、同步算法、流量调度、优先级调度，以及其他一些正在开发或集成的技术，例如"
"通信覆盖调度等）。这些技术全面地解决了跨广域分布式系统的通信问题，进一步提高了 GeoMX "
"系统训练的效率和稳健性。"

#: ../../source/index.rst:33 7200adb078194f048588fd2ebe217a8a
msgid "Optimization Techniques:"
msgstr "通信优化技术一览："

#: ../../source/index.rst:34 7b928f7b0c9641b792fed12d34744316
msgid ""
"**Bidirectional Gradient Sparsification (Bi-Sparse)**: Traditional "
"approaches such as `Deep Gradient Compression "
"<https://arxiv.org/pdf/1712.01887.pdf>`_ sparsify the pushed gradient "
"tensors. For further compression, we also sparsify the pulled "
"(aggregated) gradient tensors rather than pulling full parameters. This "
"technique is enabled between the global parameter server and the intra-"
"domain parameter servers of different data centers. Refer to `this paper "
"<https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-"
"cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_"
" for more details."
msgstr "**双向梯度稀疏化 (Bi-Sparse)**：已有方法如深度梯度压缩 (`DGC <https://arxiv.org/pdf/1712.01887.pdf>`_) "
"会对上行梯度张量进行稀疏化处理。为了进一步压缩通信流量，GeoMX 还对下行（聚合）梯度张量"
"进行稀疏化处理，而不是拉取完整模型参数。该技术被设计用于数据中心之间，以减少跨广域传输"
"的通信流量。请参考这篇文章 (`paper <https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_) "
"了解更多关于双向梯度稀疏化的介绍。"

#: ../../source/index.rst:36 d827a6445ea5450b86c296e77ccafe9c
msgid ""
"**Low-Precision Quantization (FP16)**: GeoMX also supports quantifying "
"model data at lower precision for transmission, such as in FP16 format. "
"In this scheme, GeoMX computes the model using FP32, but during "
"transmission, it converts the model data tensor into FP16. Once the "
"pulling data is received, GeoMX reverts it back into FP32 and continues "
"model computing. This effectively halves the data traffic volume over "
"both LANs and WANs."
msgstr "**低精度量化 (FP16)**：GeoMX 也支持将模型数据量化为较低精度进行传输，例如"
"以 FP16 数值精度格式。在这种方案中，GeoMX 使用 FP32 计算模型，但在传输时，它将模"
"型数据张量转换为 FP16。一旦接收到拉取的数据，GeoMX 就会将其恢复为 FP32 并继续进行"
"模型计算。这种方法可以有效地将局域网和广域网上传输的数据流量减半。"

#: ../../source/index.rst:38 ae1dc692222d417e8a727bbc34c57c70
msgid ""
"**Mixed-Precision Quantization (MPQ)**: The technology of MPQ leverages "
"both Bi-Sparse and FP16. In this scheme, tiny tensors are quantified into"
" FP16 format for transmission, while large tensors persist in the FP32 "
"format. Moreover, these large sensors will undergo a sparsification "
"process before transmission. This precaution is taken to minimize the "
"loss of crucial information and avoid significant degradation to model "
"performance."
msgstr "**混合精度量化 (MPQ)**：混合精度量化结合了双向梯度稀疏化和低精度量化两种技术。"
"在这种方案中，小张量被量化为 FP16 格式进行传输，而大张量则保持为 FP32 格式。但是，"
"这些大张量在传输前将经过稀疏化处理。这样设计是为了减少关键信息的丢失，避免对模型性能"
"造成明显损伤。"

#: ../../source/index.rst:40 91dc5c1f87fc493683fd0e6e56b2516a
msgid ""
"**Differential Gradient Transmission (DGT)**: This advanced transmission "
"protocol is optimized for distributed machine learning tasks. Leveraging "
"the tolerance of gradient descent algorithms towards partial parameter "
"loss, this protocol transfers gradients across multiple channels, each "
"with distinct levels of reliability and priority, contingent on their "
"respective contributions to model convergence. Through these prioritized "
"channels, critical gradients receive precedence in transmission, while "
"other non-important gradients are transmitted with lower priority and "
"reliability. This helps to reduce tail latency and thus reduce the end-"
"to-end transmission delay of parameter synchronization. Refer to `this "
"paper "
"<https://drive.google.com/file/d/1IbmpFybX_qXZM2g_8BrcD9IF080qci94/view>`_"
" for more details and `this repo <https://github.com/zhouhuaman/dgt>`_ "
"for individual use."
msgstr "**差异梯度传输 (DGT)**：这是一种针对分布式机器学习任务进行了特别优化的新型"
"传输协议，它利用梯度下降算法对部分梯度丢失的容忍性，使用多个具有不同可靠性和优先级的"
"通道传输梯度。梯度被调度于哪个通道进行传输，取决于它们对模型收敛的贡献。通过这些优先"
"级通道，重要梯度在传输中得到优先处理，而其他不太重要的梯度则以较低的优先级和可靠性进"
"行尽力而为传输。这有助于减少分布式机器学习通信流量的尾流时延，从而减少参数同步的完成"
"时间。关于 DGT 协议更加详细的说明请参考这篇文章"
" (`Paper <https://drive.google.com/file/d/1IbmpFybX_qXZM2g_8BrcD9IF080qci94/view>`_)，"
"如果希望独立使用 DGT 协议，请尝试这个代码库"
" (`Repo <https://github.com/zhouhuaman/dgt>`_)。"

#: ../../source/index.rst:42 363b27f16b2a4eeaa84a3813de65f49b
msgid ""
"**TSEngine**: To solve the communication in-cast issue typically "
"associated with centralized parameter servers, GeoMX incorporates "
"TSEngine, an adaptive communication scheduler designed for efficient "
"communication overlay in WANs. TSEngine dynamically optimizes the "
"topology overlay and communication logic among the training nodes in "
"response to real-time network conditions. This adaptive scheduler shows "
"significant advantages over existing communication patterns in terms of "
"system efficiency, communication, as well as scalability. Refer to `this "
"paper <https://drive.google.com/file/d/1ELfApVoCA8WCdOe3iBe-"
"VreLJCSD7r8r/view>`_ for more details and `this repo "
"<https://github.com/zhouhuaman/TSEngine>`_ for individual use."
msgstr "**TSEngine**: 为了解决分布式系统通信的 TCP Incast 问题，GeoMX 整合了"
"TSEngine，这是一个为广域网中的高效通信覆盖设计的自适应通信调度器。TSEngine 可以"
"根据实时网络条件，动态优化分布式节点之间的拓扑覆盖和通信逻辑。在系统通信效率和可扩"
"展性方面，这种自适应调度器相较于现有的通信模式展现出明显优势。请参阅此文章"
" (`Paper <https://drive.google.com/file/d/1ELfApVoCA8WCdOe3iBe-VreLJCSD7r8r/view>`_) "
"获取更多详情，也可以使用这个代码库"
" (`Repo <https://github.com/zhouhuaman/TSEngine>`_) "
"独立应用 TSEngine。"

#: ../../source/index.rst:44 fd7375c58c1e4c568ce2cbd1f39f5638
msgid ""
"**Priority-based Parameter Propagation (P3)**: In conventional "
"implementations, the gradient synchronization at round :math:`r` does not"
" overlap with the forward propagation at round :math:`r+1`, because the "
"forward propagation relies on the completion of gradient synchronization."
" To improve system efficiency, GeoMX integrates the P3 scheduler, which "
"prioritizes the transmission of shallow-layer gradients. This setup "
"enables overlapping between forward propagation and gradient "
"synchronization, allowing earlier execution of forward propagation for "
"the next round, thereby accelerating distributed training. See `this "
"paper <https://arxiv.org/pdf/1905.03960.pdf>`_ for more details and `this"
" repo <https://github.com/anandj91/p3>`_ for individual use."
msgstr "**基于优先级的参数传播 (P3)**：在传统的实现中，第 :math:`r` 轮的梯度同步与第"
" :math:`r+1` 轮的前向传播不重叠，因为前向传播依赖于梯度同步的完成。为了提高系统效率，"
"GeoMX 集成了 P3 调度器，该调度器优先传输浅层梯度。这种设置使得前向传播和梯度同步可以"
"重叠，允许更早地执行下一轮的前向传播，从而加速分布式训练。请参阅此论文"
" (`Paper <https://arxiv.org/pdf/1905.03960.pdf>`_) "
"以获取更多详情，如果希望独立使用 P3 调度器，请使用这个代码库"
" (`Repo <https://github.com/anandj91/p3>`_)。"

#: ../../source/index.rst:46 a4a2109b167c4a93b074adfc7c6fa5b1
msgid ""
"**Multi-Server Load Balancing (MultiGPS)**: GeoMX supports a balanced "
"distribution of workload, including traffic, storage, and computation, "
"across multiple global parameter servers. By preventing any single server"
" from becoming a bottleneck, MultiGPS significantly enhances efficiency, "
"scalability, and overall performance of our GeoMX system."
msgstr "**多参数服务器负载均衡 (MultiGPS)**：GeoMX 支持启用多个全局参数服务器以均衡"
"工作负载，包括通信流量、参数存储和聚合计算等。MultiGPS 可以避免单一全局参数服务器成为"
"性能瓶颈，从而提高 GeoMX 系统的效率、可扩展性和整体性能。"

#: ../../source/index.rst:49 cbcd1332aa1b49e3bb968f1d87c360c8
msgid "Synchronization Algorithms:"
msgstr "同步优化算法一览："

#: ../../source/index.rst:50 a6f66aed28c743f1b10725c34c9ccd42
msgid ""
"GeoMX supports two fundamental synchronization algorithms: a fully-"
"synchronous algorithm and a mixed-synchronous algorithm."
msgstr "GeoMX 支持两种基础的同步算法：全同步算法和混合同步算法。"

#: ../../source/index.rst:52 26f83b04d32b41cb99bb12e8d4de6a63
msgid ""
"**Fully-Synchronous Algorithm (FSA)**: In this synchronous algorithm, "
"training nodes synchronize their model data (can be parameters or "
"gradients) each round, and both parameter server systems within and "
"between data centers run in a synchronous parallel mode. This means all "
"training nodes are synchronized to ensure a consistent model. (Default)"
msgstr "**全同步算法 (FSA)**：在这种同步算法中，训练节点在每一轮都同步它们的模型"
"数据（模型参数或梯度），数据中心内部和数据中心之间的参数服务器系统都以同步并行模式"
"运行。这意味着所有训练节点的模型数据都将被同步，以确保模型的一致性。（默认设置）"

#: ../../source/index.rst:55 1833689a600647f381b30ce54444e40c
msgid ""
"NOTE: FSA is highly effective in maintaining model accuracy and "
"consistency, but at the expense of training speed, as it necessitates "
"waiting for all computations and communications to complete at every "
"iteration."
msgstr "注意：全同步算法在维护模型精度和一致性方面效果显著，但代价是牺牲训练速度，"
"因为它需要在每次迭代中都同步模型数据并等待所有计算和通信完成。"

#: ../../source/index.rst:57 e6df7504c73640368101a1a6fd5111c5
msgid ""
"**Mixed-Synchronous Algorithm (MixedSync)**: This algorithm is an "
"asynchronous version of FSA, where the difference is that the parameter "
"server system between data centers runs in an asynchronous parallel mode."
" This setup is particularly suitable for scenarios where intra-data "
"center training nodes display high homogeneity, yet there is significant "
"resource heterogeneity between different data centers. This asynchronous "
"method resolves the problem of straggling data centers, thereby "
"accelerating distributed training across WANs."
msgstr "**混合同步算法 (MixedSync)**：该算法是全同步算法的异步版本，区别在于数据中心"
"之间的参数服务器系统以异步并行模式运行。这种算法适用于数据中心内部训练节点显示出同质性，"
"但不同数据中心之间的资源异质较强的情况。异步算法解决了低性能数据中心掉队的问题，能够加速"
"广域分布式机器学习训练过程。"

#: ../../source/index.rst:60 2999eee93840435cb9c660181392f4e8
msgid ""
"NOTE: To alleviate the issue of stale gradients in asynchronous parallel "
"operations, the global parameter server can be configured to use the "
"`DCASGD <http://proceedings.mlr.press/v70/zheng17b/zheng17b.pdf>`_ "
"optimizer. This adjustment aids in improving training convergence while "
"preserving model accuracy."
msgstr "注意：为了缓解异步并行中的延迟梯度问题，我们可以配置全局参数服务器使用"
" `DCASGD <http://proceedings.mlr.press/v70/zheng17b/zheng17b.pdf>`_ "
"优化器。这个优化器可以自动补偿延迟梯度，有助于提高分布式训练的收敛性。"

#: ../../source/index.rst:62 8f9eba7ff543484ebebf82ba129fe588
msgid ""
"Building upon the two aforementioned fundamental algorithms, GeoMX also "
"offers two advanced synchronization algorithms. These two advanced "
"algorithms are specifically designed to address the challenges of "
"bandwidth scarcity and resource heterogeneity in WANs."
msgstr "基于上述两种基础同步算法，GeoMX 还提供了两种优化后的同步算法，专门设计用来"
"解决广域网中带宽稀缺和资源异质所带来的挑战。"

#: ../../source/index.rst:64 8b9c1bfd8b5e444f86c7f6d36008ccb2
msgid ""
"**Hierarchical Frequency Aggregation (HFA)**: Inspired by `this paper "
"<https://ieeexplore.ieee.org/abstract/document/9148862>`_, our HFA "
"algorithm first performs :math:`K_1` steps of local updates at the "
"training nodes, followed by :math:`K_2` steps of synchronizations at the "
"local parameter server. Finally, a global synchronization is performed at"
" the global parameter server. This approach effectively reduces the "
"frequency of model synchronization across data centers, thereby boosting "
"distributed training."
msgstr "**分层频率聚合 (HFA)**：受到这篇文章"
" (`paper <https://ieeexplore.ieee.org/abstract/document/9148862>`_)"
"的启发，我们的 HFA 算法首先在训练节点上执行 :math:`K_1` 步本地更新，然后在域"
"内参数服务器进行 :math:`K_2` 次局域同步，然后才在全局参数服务器进行一次全局同步。"
"这种方法有效减少了跨数据中心的模型同步频率，从而能够大幅提升分布式训练的效率。"

#: ../../source/index.rst:66 60010757393c4b43bf5e91766a4faae3
msgid ""
"**ESync**: Applying asynchronous algorithms to strongly heterogeneous "
"clusters can lead to severe stale gradient issues. To address this, we "
"can adopt an optimized algorithm known as ESync. ESync is a synchronous "
"parallel algorithm designed to save stragglers under conditions of strong"
" resource heterogeneity. It introduces a state server to orchestrate the "
"local iteration steps of each training node, in order to balance their "
"reach-server time (including computational and transmission time). To be "
"integrated, refer to `this paper <https://drive.google.com/file/d"
"/1bvK0EeO5vjkXveU_ccBp4Uxl-qmbmcfn/view>`_ for more details and `this "
"repo <https://github.com/Lizonghang/ESync>`_ for individual use."
msgstr "**ESync**：实践表明，将异步算法应用到强异质资源的集群会导致严重的延迟梯度问题"
"和收敛性损伤。于是，我们可以采用一种名为 ESync 的优化算法。ESync 是一种同步并行算法，"
"设计用来在强资源异质的条件下解决分布式系统中的掉队节点问题。它引入了一个状态查询服务器"
"来协调每个训练节点的本地迭代次数，以平衡他们的模型数据到达参数服务器的时间。如果您想了解"
"更多详细信息，请参考这篇文章 (`Paper <https://drive.google.com/file/d/1bvK0EeO5vjkXveU_ccBp4Uxl-qmbmcfn/view>`_)，"
"该功能正在集成中，如需提前体验请尝试这个代码库 (`Repo <https://github.com/Lizonghang/ESync>`_)。"

#: ../../source/index.rst:69 f7be0b2e43ef4cb784b8ad0ef1b8d1f6
msgid "For more details on the GeoMX system, please refer to our book:"
msgstr "如需了解更多关于 GeoMX 系统的详细信息，请参阅我们的书籍。"

#: ../../source/index.rst:71 a449ac49156347e69b3655eef7e11674
msgid ""
"**虞红芳, 李宗航, 孙罡, 罗龙. 《跨数据中心机器学习：赋能多云智能数算融合》. 电子工业出版社, 人工智能前沿理论与技术应用丛书, "
"(2023).**"
msgstr ""

#: ../../source/index.rst 9946a8827bf246a78830ba608c9edb42
msgid "Cover for GeoMX Book"
msgstr ""

