# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Zonghang Li
# This file is distributed under the same license as the GeoMX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.

msgid ""
msgstr ""
"Project-Id-Version: GeoMX 1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-02 09:50+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Zonghang Li <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <lizhuestc@gmail.com>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/index.rst:11
msgid "GeoMX Tutorials"
msgstr "GeoMX 教程"

#: ../../source/index.rst:7 ee1a1e4273a34dcea8ee2b229efe3e28
msgid "Welcome to GeoMX Docs! 😁"
msgstr "欢迎来到 GeoMX 的中文文档！😁"

#: ../../source/index.rst:26 32bf00cb9bc74913bbf629a4d0ec3647
msgid ""
"**GeoMX** is a fast and unified distributed system for training ML "
"algorithms over geographical data centers. Built upon the `MXNET "
"<https://github.com/apache/mxnet>`_ framework, GeoMX integrates several "
"sophisticated optimization techniques to enhance its training efficiency."
" These strategic enhancements result in a significant performance boost "
"compared to the original MXNET system, offering 20x acceleration under "
"identical network bandwidth conditions. This superior efficiency propels "
"GeoMX to the forefront of training systems used for geographically "
"dispersed data centers, showcasing satisfying performance and "
"effectiveness."
msgstr "GeoMX 是一个高性能分布式机器学习系统，它用于在地理分散的多个数据中心之间"
"进行协同的机器学习模型训练。GeoMX 是基于 `MXNET <https://github.com/apache/mxnet>`_ "
"系统建立的，并且融合了一系列复杂的优化技术以提高训练效率。通过这些改进，相比于原始的 MXNET "
"系统，在相同网络带宽的条件下，GeoMX 的训练速度提升了 20 倍。这种显著的效率提升使得 GeoMX "
"适用于地理分散的数据中心场景，在跨广域的分布式机器学习应用中表现出了优越的性能和效率。"

#: ../../source/index.rst:28 284dc6e7dd40416b8252f441ed8a2f57
msgid ""
"GeoMX employs the `Hierarchical Parameter Server (HiPS) "
"<https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-"
"cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_"
" framework as its fundamental training architecture, designed to "
"segregate the network environment within and beyond the data center. This"
" unique architecture includes an intra-domain parameter server system "
"within each data center while concurrently establishing an inter-"
"parameter server connection between different data centers. In this "
"configuration, model data traffic undergoes two stages of aggregation: an"
" initial local aggregation within each data center, followed by a global "
"aggregation at the central data center. This approach effectively "
"minimizes cross-WAN traffic and, consequently, reduces communication "
"overhead."
msgstr "GeoMX 采用一种名为分层参数服务器 (`HiPS <https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_) "
"的新型架构作为它的基础训练架构。HiPS 架构分隔了数据中心内外部网络环境，它在每个数据"
"中心内部建立域内参数服务器系统，同时在不同的数据中心之间也建立起了全局参数服务器系统。"
"在这种设置下，模型数据的传输会经历两个阶段的聚合：首先，在各自的数据中心内部局部聚合，"
"然后，在全局数据中心进行全局聚合。这种方法有效减小了跨广域网的通信流量，从而显著降低通信开销。"

#: ../../source/index.rst:30 0b759d2159d244808f1b652d2b99f79c
msgid ""
"But it's far from enough, given the often limited and varied network "
"conditions in WANs, distributed training across data centers can "
"potentially create communication bottlenecks. To mitigate these issues, "
"GeoMX employs a variety of optimization techniques. These include "
"gradient sparsification, low-precision quantization (e.g., fp16), mixed-"
"precision quantization, advanced transmission protocols, synchronization "
"algorithms, flow scheduling, and priority scheduling, among others (e.g.,"
" overlay scheduling, currently in development). These techniques "
"comprehensively tackle communication issues, further enhancing the "
"efficiency and robustness of distributed machine learning training in "
"GeoMX."
msgstr "但是，仅有这些还远远不够。考虑到广域网中的网络资源常常是受限的，并且随时间"
"动态时变，跨数据中心的分布式训练仍然面临诸多通信瓶颈。为了缓解这些瓶颈，GeoMX 采用"
"了多种优化技术。这些技术包括梯度稀疏化、低精度量化（例如 FP16）、混合精度量化、更先"
"进的传输协议、同步算法、流量调度、优先级调度，以及其他一些正在开发或集成的技术，例如"
"通信覆盖调度等）。这些技术全面地解决了跨广域分布式系统的通信问题，进一步提高了 GeoMX "
"系统训练的效率和稳健性。"

#: ../../source/index.rst:33 7200adb078194f048588fd2ebe217a8a
msgid "Optimization Techniques:"
msgstr "优化技术一览："

#: ../../source/index.rst:34 7b928f7b0c9641b792fed12d34744316
msgid ""
"**Bidirectional Gradient Sparsification (Bi-Sparse)**: Traditional "
"approaches such as `Deep Gradient Compression "
"<https://arxiv.org/pdf/1712.01887.pdf>`_ sparsify the pushed gradient "
"tensors. For further compression, we also sparsify the pulled "
"(aggregated) gradient tensors rather than pulling full parameters. This "
"technique is enabled between the global parameter server and the intra-"
"domain parameter servers of different data centers. Refer to `this paper "
"<https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-"
"cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_"
" for more details."
msgstr "**双向梯度稀疏化 (Bi-Sparse)**："

#: ../../source/index.rst:36 d827a6445ea5450b86c296e77ccafe9c
msgid ""
"**Low-Precision Quantization (FP16)**: GeoMX also supports quantifying "
"model data at lower precision for transmission, such as in FP16 format. "
"In this scheme, GeoMX computes the model using FP32, but during "
"transmission, it converts the model data tensor into FP16. Once the "
"pulling data is received, GeoMX reverts it back into FP32 and continues "
"model computing. This effectively halves the data traffic volume over "
"both LANs and WANs."
msgstr ""

#: ../../source/index.rst:38 ae1dc692222d417e8a727bbc34c57c70
msgid ""
"**Mixed-Precision Quantization (MPQ)**: The technology of MPQ leverages "
"both Bi-Sparse and FP16. In this scheme, tiny tensors are quantified into"
" FP16 format for transmission, while large tensors persist in the FP32 "
"format. Moreover, these large sensors will undergo a sparsification "
"process before transmission. This precaution is taken to minimize the "
"loss of crucial information and avoid significant degradation to model "
"performance."
msgstr ""

#: ../../source/index.rst:40 91dc5c1f87fc493683fd0e6e56b2516a
msgid ""
"**Differential Gradient Transmission (DGT)**: This advanced transmission "
"protocol is optimized for distributed machine learning tasks. Leveraging "
"the tolerance of gradient descent algorithms towards partial parameter "
"loss, this protocol transfers gradients across multiple channels, each "
"with distinct levels of reliability and priority, contingent on their "
"respective contributions to model convergence. Through these prioritized "
"channels, critical gradients receive precedence in transmission, while "
"other non-important gradients are transmitted with lower priority and "
"reliability. This helps to reduce tail latency and thus reduce the end-"
"to-end transmission delay of parameter synchronization. Refer to `this "
"paper "
"<https://drive.google.com/file/d/1IbmpFybX_qXZM2g_8BrcD9IF080qci94/view>`_"
" for more details and `this repo <https://github.com/zhouhuaman/dgt>`_ "
"for individual use."
msgstr ""

#: ../../source/index.rst:42 363b27f16b2a4eeaa84a3813de65f49b
msgid ""
"**TSEngine**: To solve the communication in-cast issue typically "
"associated with centralized parameter servers, GeoMX incorporates "
"TSEngine, an adaptive communication scheduler designed for efficient "
"communication overlay in WANs. TSEngine dynamically optimizes the "
"topology overlay and communication logic among the training nodes in "
"response to real-time network conditions. This adaptive scheduler shows "
"significant advantages over existing communication patterns in terms of "
"system efficiency, communication, as well as scalability. Refer to `this "
"paper <https://drive.google.com/file/d/1ELfApVoCA8WCdOe3iBe-"
"VreLJCSD7r8r/view>`_ for more details and `this repo "
"<https://github.com/zhouhuaman/TSEngine>`_ for individual use."
msgstr ""

#: ../../source/index.rst:44 fd7375c58c1e4c568ce2cbd1f39f5638
msgid ""
"**Priority-based Parameter Propagation (P3)**: In conventional "
"implementations, the gradient synchronization at round :math:`r` does not"
" overlap with the forward propagation at round :math:`r+1`, because the "
"forward propagation relies on the completion of gradient synchronization."
" To improve system efficiency, GeoMX integrates the P3 scheduler, which "
"prioritizes the transmission of shallow-layer gradients. This setup "
"enables overlapping between forward propagation and gradient "
"synchronization, allowing earlier execution of forward propagation for "
"the next round, thereby accelerating distributed training. See `this "
"paper <https://arxiv.org/pdf/1905.03960.pdf>`_ for more details and `this"
" repo <https://github.com/anandj91/p3>`_ for individual use."
msgstr ""

#: ../../source/index.rst:46 a4a2109b167c4a93b074adfc7c6fa5b1
msgid ""
"**Multi-Server Load Balancing (MultiGPS)**: GeoMX supports a balanced "
"distribution of workload, including traffic, storage, and computation, "
"across multiple global parameter servers. By preventing any single server"
" from becoming a bottleneck, MultiGPS significantly enhances efficiency, "
"scalability, and overall performance of our GeoMX system."
msgstr ""

#: ../../source/index.rst:49 cbcd1332aa1b49e3bb968f1d87c360c8
msgid "Synchronization Algorithms:"
msgstr ""

#: ../../source/index.rst:50 a6f66aed28c743f1b10725c34c9ccd42
msgid ""
"GeoMX supports two fundamental synchronization algorithms: a fully-"
"synchronous algorithm and a mixed-synchronous algorithm."
msgstr ""

#: ../../source/index.rst:52 26f83b04d32b41cb99bb12e8d4de6a63
msgid ""
"**Fully-Synchronous Algorithm (FSA)**: In this synchronous algorithm, "
"training nodes synchronize their model data (can be parameters or "
"gradients) each round, and both parameter server systems within and "
"between data centers run in a synchronous parallel mode. This means all "
"training nodes are synchronized to ensure a consistent model. (Default)"
msgstr ""

#: ../../source/index.rst:55 1833689a600647f381b30ce54444e40c
msgid ""
"NOTE: FSA is highly effective in maintaining model accuracy and "
"consistency, but at the expense of training speed, as it necessitates "
"waiting for all computations and communications to complete at every "
"iteration."
msgstr ""

#: ../../source/index.rst:57 e6df7504c73640368101a1a6fd5111c5
msgid ""
"**Mixed-Synchronous Algorithm (MixedSync)**: This algorithm is an "
"asynchronous version of FSA, where the difference is that the parameter "
"server system between data centers runs in an asynchronous parallel mode."
" This setup is particularly suitable for scenarios where intra-data "
"center training nodes display high homogeneity, yet there is significant "
"resource heterogeneity between different data centers. This asynchronous "
"method resolves the problem of straggling data centers, thereby "
"accelerating distributed training across WANs."
msgstr ""

#: ../../source/index.rst:60 2999eee93840435cb9c660181392f4e8
msgid ""
"NOTE: To alleviate the issue of stale gradients in asynchronous parallel "
"operations, the global parameter server can be configured to use the "
"`DCASGD <http://proceedings.mlr.press/v70/zheng17b/zheng17b.pdf>`_ "
"optimizer. This adjustment aids in improving training convergence while "
"preserving model accuracy."
msgstr ""

#: ../../source/index.rst:62 8f9eba7ff543484ebebf82ba129fe588
msgid ""
"Building upon the two aforementioned fundamental algorithms, GeoMX also "
"offers two advanced synchronization algorithms. These two advanced "
"algorithms are specifically designed to address the challenges of "
"bandwidth scarcity and resource heterogeneity in WANs."
msgstr ""

#: ../../source/index.rst:64 8b9c1bfd8b5e444f86c7f6d36008ccb2
msgid ""
"**Hierarchical Frequency Aggregation (HFA)**: Inspired by `this paper "
"<https://ieeexplore.ieee.org/abstract/document/9148862>`_, our HFA "
"algorithm first performs :math:`K_1` steps of local updates at the "
"training nodes, followed by :math:`K_2` steps of synchronizations at the "
"local parameter server. Finally, a global synchronization is performed at"
" the global parameter server. This approach effectively reduces the "
"frequency of model synchronization across data centers, thereby boosting "
"distributed training."
msgstr ""

#: ../../source/index.rst:66 60010757393c4b43bf5e91766a4faae3
msgid ""
"**ESync**: Applying asynchronous algorithms to strongly heterogeneous "
"clusters can lead to severe stale gradient issues. To address this, we "
"can adopt an optimized algorithm known as ESync. ESync is a synchronous "
"parallel algorithm designed to save stragglers under conditions of strong"
" resource heterogeneity. It introduces a state server to orchestrate the "
"local iteration steps of each training node, in order to balance their "
"reach-server time (including computational and transmission time). To be "
"integrated, refer to `this paper <https://drive.google.com/file/d"
"/1bvK0EeO5vjkXveU_ccBp4Uxl-qmbmcfn/view>`_ for more details and `this "
"repo <https://github.com/Lizonghang/ESync>`_ for individual use."
msgstr ""

#: ../../source/index.rst:69 f7be0b2e43ef4cb784b8ad0ef1b8d1f6
msgid "For more details on the GeoMX system, please refer to our book:"
msgstr ""

#: ../../source/index.rst:71 a449ac49156347e69b3655eef7e11674
msgid ""
"**虞红芳, 李宗航, 孙罡, 罗龙. 《跨数据中心机器学习：赋能多云智能数算融合》. 电子工业出版社, 人工智能前沿理论与技术应用丛书, "
"(2023).**"
msgstr ""

#: ../../source/index.rst 9946a8827bf246a78830ba608c9edb42
msgid "Cover for GeoMX Book"
msgstr ""

