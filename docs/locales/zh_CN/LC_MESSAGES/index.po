# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Zonghang Li
# This file is distributed under the same license as the GeoMX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.

msgid ""
msgstr ""
"Project-Id-Version: GeoMX 1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-02 09:50+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Li, Zonghang <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <lizhuestc@gmail.com>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/index.rst:11
msgid "GeoMX Tutorials"
msgstr "GeoMX æ•™ç¨‹"

#: ../../source/index.rst:7 ee1a1e4273a34dcea8ee2b229efe3e28
msgid "Welcome to GeoMX Docs! ğŸ˜"
msgstr "æ¬¢è¿æ¥åˆ° GeoMX çš„ä¸­æ–‡æ–‡æ¡£ï¼ğŸ˜"

#: ../../source/index.rst:26 32bf00cb9bc74913bbf629a4d0ec3647
msgid ""
"**GeoMX** is a fast and unified distributed system for training ML "
"algorithms over geographical data centers. Built upon the `MXNET "
"<https://github.com/apache/mxnet>`_ framework, GeoMX integrates several "
"sophisticated optimization techniques to enhance its training efficiency."
" These strategic enhancements result in a significant performance boost "
"compared to the original MXNET system, offering 20x acceleration under "
"identical network bandwidth conditions. This superior efficiency propels "
"GeoMX to the forefront of training systems used for geographically "
"dispersed data centers, showcasing satisfying performance and "
"effectiveness."
msgstr "GeoMX æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼Œå®ƒç”¨äºåœ¨åœ°ç†åˆ†æ•£çš„å¤šä¸ªæ•°æ®ä¸­å¿ƒä¹‹é—´"
"è¿›è¡ŒååŒçš„æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒã€‚GeoMX æ˜¯åŸºäº `MXNET <https://github.com/apache/mxnet>`_ "
"ç³»ç»Ÿå»ºç«‹çš„ï¼Œå¹¶ä¸”èåˆäº†ä¸€ç³»åˆ—å¤æ‚çš„ä¼˜åŒ–æŠ€æœ¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡è¿™äº›æ”¹è¿›ï¼Œç›¸æ¯”äºåŸå§‹çš„ MXNET "
"ç³»ç»Ÿï¼Œåœ¨ç›¸åŒç½‘ç»œå¸¦å®½çš„æ¡ä»¶ä¸‹ï¼ŒGeoMX çš„è®­ç»ƒé€Ÿåº¦æå‡äº† 20 å€ã€‚è¿™ç§æ˜¾è‘—çš„æ•ˆç‡æå‡ä½¿å¾— GeoMX "
"é€‚ç”¨äºåœ°ç†åˆ†æ•£çš„æ•°æ®ä¸­å¿ƒåœºæ™¯ï¼Œåœ¨è·¨å¹¿åŸŸçš„åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ åº”ç”¨ä¸­è¡¨ç°å‡ºäº†ä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚"

#: ../../source/index.rst:28 284dc6e7dd40416b8252f441ed8a2f57
msgid ""
"GeoMX employs the `Hierarchical Parameter Server (HiPS) "
"<https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-"
"cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_"
" framework as its fundamental training architecture, designed to "
"segregate the network environment within and beyond the data center. This"
" unique architecture includes an intra-domain parameter server system "
"within each data center while concurrently establishing an inter-"
"parameter server connection between different data centers. In this "
"configuration, model data traffic undergoes two stages of aggregation: an"
" initial local aggregation within each data center, followed by a global "
"aggregation at the central data center. This approach effectively "
"minimizes cross-WAN traffic and, consequently, reduces communication "
"overhead."
msgstr "GeoMX é‡‡ç”¨ä¸€ç§åä¸ºåˆ†å±‚å‚æ•°æœåŠ¡å™¨ (`HiPS <https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_) "
"çš„æ–°å‹æ¶æ„ä½œä¸ºå®ƒçš„åŸºç¡€è®­ç»ƒæ¶æ„ã€‚HiPS æ¶æ„åˆ†éš”äº†æ•°æ®ä¸­å¿ƒå†…å¤–éƒ¨ç½‘ç»œç¯å¢ƒï¼Œå®ƒåœ¨æ¯ä¸ªæ•°æ®"
"ä¸­å¿ƒå†…éƒ¨å»ºç«‹åŸŸå†…å‚æ•°æœåŠ¡å™¨ç³»ç»Ÿï¼ŒåŒæ—¶åœ¨ä¸åŒçš„æ•°æ®ä¸­å¿ƒä¹‹é—´ä¹Ÿå»ºç«‹èµ·äº†å…¨å±€å‚æ•°æœåŠ¡å™¨ç³»ç»Ÿã€‚"
"åœ¨è¿™ç§è®¾ç½®ä¸‹ï¼Œæ¨¡å‹æ•°æ®çš„ä¼ è¾“ä¼šç»å†ä¸¤ä¸ªé˜¶æ®µçš„èšåˆï¼šé¦–å…ˆï¼Œåœ¨å„è‡ªçš„æ•°æ®ä¸­å¿ƒå†…éƒ¨å±€éƒ¨èšåˆï¼Œ"
"ç„¶åï¼Œåœ¨å…¨å±€æ•°æ®ä¸­å¿ƒè¿›è¡Œå…¨å±€èšåˆã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆå‡å°äº†è·¨å¹¿åŸŸç½‘çš„é€šä¿¡æµé‡ï¼Œä»è€Œæ˜¾è‘—é™ä½é€šä¿¡å¼€é”€ã€‚"

#: ../../source/index.rst:30 0b759d2159d244808f1b652d2b99f79c
msgid ""
"But it's far from enough, given the often limited and varied network "
"conditions in WANs, distributed training across data centers can "
"potentially create communication bottlenecks. To mitigate these issues, "
"GeoMX employs a variety of optimization techniques. These include "
"gradient sparsification, low-precision quantization (e.g., fp16), mixed-"
"precision quantization, advanced transmission protocols, synchronization "
"algorithms, flow scheduling, and priority scheduling, among others (e.g.,"
" overlay scheduling, currently in development). These techniques "
"comprehensively tackle communication issues, further enhancing the "
"efficiency and robustness of distributed machine learning training in "
"GeoMX."
msgstr "ä½†æ˜¯ï¼Œä»…æœ‰è¿™äº›è¿˜è¿œè¿œä¸å¤Ÿã€‚è€ƒè™‘åˆ°å¹¿åŸŸç½‘ä¸­çš„ç½‘ç»œèµ„æºå¸¸å¸¸æ˜¯å—é™çš„ï¼Œå¹¶ä¸”éšæ—¶é—´"
"åŠ¨æ€æ—¶å˜ï¼Œè·¨æ•°æ®ä¸­å¿ƒçš„åˆ†å¸ƒå¼è®­ç»ƒä»ç„¶é¢ä¸´è¯¸å¤šé€šä¿¡ç“¶é¢ˆã€‚ä¸ºäº†ç¼“è§£è¿™äº›ç“¶é¢ˆï¼ŒGeoMX é‡‡ç”¨"
"äº†å¤šç§ä¼˜åŒ–æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯åŒ…æ‹¬æ¢¯åº¦ç¨€ç–åŒ–ã€ä½ç²¾åº¦é‡åŒ–ï¼ˆä¾‹å¦‚ FP16ï¼‰ã€æ··åˆç²¾åº¦é‡åŒ–ã€æ›´å…ˆ"
"è¿›çš„ä¼ è¾“åè®®ã€åŒæ­¥ç®—æ³•ã€æµé‡è°ƒåº¦ã€ä¼˜å…ˆçº§è°ƒåº¦ï¼Œä»¥åŠå…¶ä»–ä¸€äº›æ­£åœ¨å¼€å‘æˆ–é›†æˆçš„æŠ€æœ¯ï¼Œä¾‹å¦‚"
"é€šä¿¡è¦†ç›–è°ƒåº¦ç­‰ï¼‰ã€‚è¿™äº›æŠ€æœ¯å…¨é¢åœ°è§£å†³äº†è·¨å¹¿åŸŸåˆ†å¸ƒå¼ç³»ç»Ÿçš„é€šä¿¡é—®é¢˜ï¼Œè¿›ä¸€æ­¥æé«˜äº† GeoMX "
"ç³»ç»Ÿè®­ç»ƒçš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚"

#: ../../source/index.rst:33 7200adb078194f048588fd2ebe217a8a
msgid "Optimization Techniques:"
msgstr "é€šä¿¡ä¼˜åŒ–æŠ€æœ¯ä¸€è§ˆï¼š"

#: ../../source/index.rst:34 7b928f7b0c9641b792fed12d34744316
msgid ""
"**Bidirectional Gradient Sparsification (Bi-Sparse)**: Traditional "
"approaches such as `Deep Gradient Compression "
"<https://arxiv.org/pdf/1712.01887.pdf>`_ sparsify the pushed gradient "
"tensors. For further compression, we also sparsify the pulled "
"(aggregated) gradient tensors rather than pulling full parameters. This "
"technique is enabled between the global parameter server and the intra-"
"domain parameter servers of different data centers. Refer to `this paper "
"<https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-"
"cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_"
" for more details."
msgstr "**åŒå‘æ¢¯åº¦ç¨€ç–åŒ– (Bi-Sparse)**ï¼šå·²æœ‰æ–¹æ³•å¦‚æ·±åº¦æ¢¯åº¦å‹ç¼© (`DGC <https://arxiv.org/pdf/1712.01887.pdf>`_) "
"ä¼šå¯¹ä¸Šè¡Œæ¢¯åº¦å¼ é‡è¿›è¡Œç¨€ç–åŒ–å¤„ç†ã€‚ä¸ºäº†è¿›ä¸€æ­¥å‹ç¼©é€šä¿¡æµé‡ï¼ŒGeoMX è¿˜å¯¹ä¸‹è¡Œï¼ˆèšåˆï¼‰æ¢¯åº¦å¼ é‡"
"è¿›è¡Œç¨€ç–åŒ–å¤„ç†ï¼Œè€Œä¸æ˜¯æ‹‰å–å®Œæ•´æ¨¡å‹å‚æ•°ã€‚è¯¥æŠ€æœ¯è¢«è®¾è®¡ç”¨äºæ•°æ®ä¸­å¿ƒä¹‹é—´ï¼Œä»¥å‡å°‘è·¨å¹¿åŸŸä¼ è¾“"
"çš„é€šä¿¡æµé‡ã€‚è¯·å‚è€ƒè¿™ç¯‡æ–‡ç«  (`paper <https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_) "
"äº†è§£æ›´å¤šå…³äºåŒå‘æ¢¯åº¦ç¨€ç–åŒ–çš„ä»‹ç»ã€‚"

#: ../../source/index.rst:36 d827a6445ea5450b86c296e77ccafe9c
msgid ""
"**Low-Precision Quantization (FP16)**: GeoMX also supports quantifying "
"model data at lower precision for transmission, such as in FP16 format. "
"In this scheme, GeoMX computes the model using FP32, but during "
"transmission, it converts the model data tensor into FP16. Once the "
"pulling data is received, GeoMX reverts it back into FP32 and continues "
"model computing. This effectively halves the data traffic volume over "
"both LANs and WANs."
msgstr "**ä½ç²¾åº¦é‡åŒ– (FP16)**ï¼šGeoMX ä¹Ÿæ”¯æŒå°†æ¨¡å‹æ•°æ®é‡åŒ–ä¸ºè¾ƒä½ç²¾åº¦è¿›è¡Œä¼ è¾“ï¼Œä¾‹å¦‚"
"ä»¥ FP16 æ•°å€¼ç²¾åº¦æ ¼å¼ã€‚åœ¨è¿™ç§æ–¹æ¡ˆä¸­ï¼ŒGeoMX ä½¿ç”¨ FP32 è®¡ç®—æ¨¡å‹ï¼Œä½†åœ¨ä¼ è¾“æ—¶ï¼Œå®ƒå°†æ¨¡"
"å‹æ•°æ®å¼ é‡è½¬æ¢ä¸º FP16ã€‚ä¸€æ—¦æ¥æ”¶åˆ°æ‹‰å–çš„æ•°æ®ï¼ŒGeoMX å°±ä¼šå°†å…¶æ¢å¤ä¸º FP32 å¹¶ç»§ç»­è¿›è¡Œ"
"æ¨¡å‹è®¡ç®—ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†å±€åŸŸç½‘å’Œå¹¿åŸŸç½‘ä¸Šä¼ è¾“çš„æ•°æ®æµé‡å‡åŠã€‚"

#: ../../source/index.rst:38 ae1dc692222d417e8a727bbc34c57c70
msgid ""
"**Mixed-Precision Quantization (MPQ)**: The technology of MPQ leverages "
"both Bi-Sparse and FP16. In this scheme, tiny tensors are quantified into"
" FP16 format for transmission, while large tensors persist in the FP32 "
"format. Moreover, these large sensors will undergo a sparsification "
"process before transmission. This precaution is taken to minimize the "
"loss of crucial information and avoid significant degradation to model "
"performance."
msgstr "**æ··åˆç²¾åº¦é‡åŒ– (MPQ)**ï¼šæ··åˆç²¾åº¦é‡åŒ–ç»“åˆäº†åŒå‘æ¢¯åº¦ç¨€ç–åŒ–å’Œä½ç²¾åº¦é‡åŒ–ä¸¤ç§æŠ€æœ¯ã€‚"
"åœ¨è¿™ç§æ–¹æ¡ˆä¸­ï¼Œå°å¼ é‡è¢«é‡åŒ–ä¸º FP16 æ ¼å¼è¿›è¡Œä¼ è¾“ï¼Œè€Œå¤§å¼ é‡åˆ™ä¿æŒä¸º FP32 æ ¼å¼ã€‚ä½†æ˜¯ï¼Œ"
"è¿™äº›å¤§å¼ é‡åœ¨ä¼ è¾“å‰å°†ç»è¿‡ç¨€ç–åŒ–å¤„ç†ã€‚è¿™æ ·è®¾è®¡æ˜¯ä¸ºäº†å‡å°‘å…³é”®ä¿¡æ¯çš„ä¸¢å¤±ï¼Œé¿å…å¯¹æ¨¡å‹æ€§èƒ½"
"é€ æˆæ˜æ˜¾æŸä¼¤ã€‚"

#: ../../source/index.rst:40 91dc5c1f87fc493683fd0e6e56b2516a
msgid ""
"**Differential Gradient Transmission (DGT)**: This advanced transmission "
"protocol is optimized for distributed machine learning tasks. Leveraging "
"the tolerance of gradient descent algorithms towards partial parameter "
"loss, this protocol transfers gradients across multiple channels, each "
"with distinct levels of reliability and priority, contingent on their "
"respective contributions to model convergence. Through these prioritized "
"channels, critical gradients receive precedence in transmission, while "
"other non-important gradients are transmitted with lower priority and "
"reliability. This helps to reduce tail latency and thus reduce the end-"
"to-end transmission delay of parameter synchronization. Refer to `this "
"paper "
"<https://drive.google.com/file/d/1IbmpFybX_qXZM2g_8BrcD9IF080qci94/view>`_"
" for more details and `this repo <https://github.com/zhouhuaman/dgt>`_ "
"for individual use."
msgstr "**å·®å¼‚æ¢¯åº¦ä¼ è¾“ (DGT)**ï¼šè¿™æ˜¯ä¸€ç§é’ˆå¯¹åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ä»»åŠ¡è¿›è¡Œäº†ç‰¹åˆ«ä¼˜åŒ–çš„æ–°å‹"
"ä¼ è¾“åè®®ï¼Œå®ƒåˆ©ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•å¯¹éƒ¨åˆ†æ¢¯åº¦ä¸¢å¤±çš„å®¹å¿æ€§ï¼Œä½¿ç”¨å¤šä¸ªå…·æœ‰ä¸åŒå¯é æ€§å’Œä¼˜å…ˆçº§çš„"
"é€šé“ä¼ è¾“æ¢¯åº¦ã€‚æ¢¯åº¦è¢«è°ƒåº¦äºå“ªä¸ªé€šé“è¿›è¡Œä¼ è¾“ï¼Œå–å†³äºå®ƒä»¬å¯¹æ¨¡å‹æ”¶æ•›çš„è´¡çŒ®ã€‚é€šè¿‡è¿™äº›ä¼˜å…ˆ"
"çº§é€šé“ï¼Œé‡è¦æ¢¯åº¦åœ¨ä¼ è¾“ä¸­å¾—åˆ°ä¼˜å…ˆå¤„ç†ï¼Œè€Œå…¶ä»–ä¸å¤ªé‡è¦çš„æ¢¯åº¦åˆ™ä»¥è¾ƒä½çš„ä¼˜å…ˆçº§å’Œå¯é æ€§è¿›"
"è¡Œå°½åŠ›è€Œä¸ºä¼ è¾“ã€‚è¿™æœ‰åŠ©äºå‡å°‘åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ é€šä¿¡æµé‡çš„å°¾æµæ—¶å»¶ï¼Œä»è€Œå‡å°‘å‚æ•°åŒæ­¥çš„å®Œæˆ"
"æ—¶é—´ã€‚å…³äº DGT åè®®æ›´åŠ è¯¦ç»†çš„è¯´æ˜è¯·å‚è€ƒè¿™ç¯‡æ–‡ç« "
" (`Paper <https://drive.google.com/file/d/1IbmpFybX_qXZM2g_8BrcD9IF080qci94/view>`_)ï¼Œ"
"å¦‚æœå¸Œæœ›ç‹¬ç«‹ä½¿ç”¨ DGT åè®®ï¼Œè¯·å°è¯•è¿™ä¸ªä»£ç åº“"
" (`Repo <https://github.com/zhouhuaman/dgt>`_)ã€‚"

#: ../../source/index.rst:42 363b27f16b2a4eeaa84a3813de65f49b
msgid ""
"**TSEngine**: To solve the communication in-cast issue typically "
"associated with centralized parameter servers, GeoMX incorporates "
"TSEngine, an adaptive communication scheduler designed for efficient "
"communication overlay in WANs. TSEngine dynamically optimizes the "
"topology overlay and communication logic among the training nodes in "
"response to real-time network conditions. This adaptive scheduler shows "
"significant advantages over existing communication patterns in terms of "
"system efficiency, communication, as well as scalability. Refer to `this "
"paper <https://drive.google.com/file/d/1ELfApVoCA8WCdOe3iBe-"
"VreLJCSD7r8r/view>`_ for more details and `this repo "
"<https://github.com/zhouhuaman/TSEngine>`_ for individual use."
msgstr "**TSEngine**: ä¸ºäº†è§£å†³åˆ†å¸ƒå¼ç³»ç»Ÿé€šä¿¡çš„ TCP Incast é—®é¢˜ï¼ŒGeoMX æ•´åˆäº†"
"TSEngineï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¹¿åŸŸç½‘ä¸­çš„é«˜æ•ˆé€šä¿¡è¦†ç›–è®¾è®¡çš„è‡ªé€‚åº”é€šä¿¡è°ƒåº¦å™¨ã€‚TSEngine å¯ä»¥"
"æ ¹æ®å®æ—¶ç½‘ç»œæ¡ä»¶ï¼ŒåŠ¨æ€ä¼˜åŒ–åˆ†å¸ƒå¼èŠ‚ç‚¹ä¹‹é—´çš„æ‹“æ‰‘è¦†ç›–å’Œé€šä¿¡é€»è¾‘ã€‚åœ¨ç³»ç»Ÿé€šä¿¡æ•ˆç‡å’Œå¯æ‰©"
"å±•æ€§æ–¹é¢ï¼Œè¿™ç§è‡ªé€‚åº”è°ƒåº¦å™¨ç›¸è¾ƒäºç°æœ‰çš„é€šä¿¡æ¨¡å¼å±•ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚è¯·å‚é˜…æ­¤æ–‡ç« "
" (`Paper <https://drive.google.com/file/d/1ELfApVoCA8WCdOe3iBe-VreLJCSD7r8r/view>`_) "
"è·å–æ›´å¤šè¯¦æƒ…ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è¿™ä¸ªä»£ç åº“"
" (`Repo <https://github.com/zhouhuaman/TSEngine>`_) "
"ç‹¬ç«‹åº”ç”¨ TSEngineã€‚"

#: ../../source/index.rst:44 fd7375c58c1e4c568ce2cbd1f39f5638
msgid ""
"**Priority-based Parameter Propagation (P3)**: In conventional "
"implementations, the gradient synchronization at round :math:`r` does not"
" overlap with the forward propagation at round :math:`r+1`, because the "
"forward propagation relies on the completion of gradient synchronization."
" To improve system efficiency, GeoMX integrates the P3 scheduler, which "
"prioritizes the transmission of shallow-layer gradients. This setup "
"enables overlapping between forward propagation and gradient "
"synchronization, allowing earlier execution of forward propagation for "
"the next round, thereby accelerating distributed training. See `this "
"paper <https://arxiv.org/pdf/1905.03960.pdf>`_ for more details and `this"
" repo <https://github.com/anandj91/p3>`_ for individual use."
msgstr "**åŸºäºä¼˜å…ˆçº§çš„å‚æ•°ä¼ æ’­ (P3)**ï¼šåœ¨ä¼ ç»Ÿçš„å®ç°ä¸­ï¼Œç¬¬ :math:`r` è½®çš„æ¢¯åº¦åŒæ­¥ä¸ç¬¬"
" :math:`r+1` è½®çš„å‰å‘ä¼ æ’­ä¸é‡å ï¼Œå› ä¸ºå‰å‘ä¼ æ’­ä¾èµ–äºæ¢¯åº¦åŒæ­¥çš„å®Œæˆã€‚ä¸ºäº†æé«˜ç³»ç»Ÿæ•ˆç‡ï¼Œ"
"GeoMX é›†æˆäº† P3 è°ƒåº¦å™¨ï¼Œè¯¥è°ƒåº¦å™¨ä¼˜å…ˆä¼ è¾“æµ…å±‚æ¢¯åº¦ã€‚è¿™ç§è®¾ç½®ä½¿å¾—å‰å‘ä¼ æ’­å’Œæ¢¯åº¦åŒæ­¥å¯ä»¥"
"é‡å ï¼Œå…è®¸æ›´æ—©åœ°æ‰§è¡Œä¸‹ä¸€è½®çš„å‰å‘ä¼ æ’­ï¼Œä»è€ŒåŠ é€Ÿåˆ†å¸ƒå¼è®­ç»ƒã€‚è¯·å‚é˜…æ­¤è®ºæ–‡"
" (`Paper <https://arxiv.org/pdf/1905.03960.pdf>`_) "
"ä»¥è·å–æ›´å¤šè¯¦æƒ…ï¼Œå¦‚æœå¸Œæœ›ç‹¬ç«‹ä½¿ç”¨ P3 è°ƒåº¦å™¨ï¼Œè¯·ä½¿ç”¨è¿™ä¸ªä»£ç åº“"
" (`Repo <https://github.com/anandj91/p3>`_)ã€‚"

#: ../../source/index.rst:46 a4a2109b167c4a93b074adfc7c6fa5b1
msgid ""
"**Multi-Server Load Balancing (MultiGPS)**: GeoMX supports a balanced "
"distribution of workload, including traffic, storage, and computation, "
"across multiple global parameter servers. By preventing any single server"
" from becoming a bottleneck, MultiGPS significantly enhances efficiency, "
"scalability, and overall performance of our GeoMX system."
msgstr "**å¤šå‚æ•°æœåŠ¡å™¨è´Ÿè½½å‡è¡¡ (MultiGPS)**ï¼šGeoMX æ”¯æŒå¯ç”¨å¤šä¸ªå…¨å±€å‚æ•°æœåŠ¡å™¨ä»¥å‡è¡¡"
"å·¥ä½œè´Ÿè½½ï¼ŒåŒ…æ‹¬é€šä¿¡æµé‡ã€å‚æ•°å­˜å‚¨å’Œèšåˆè®¡ç®—ç­‰ã€‚MultiGPS å¯ä»¥é¿å…å•ä¸€å…¨å±€å‚æ•°æœåŠ¡å™¨æˆä¸º"
"æ€§èƒ½ç“¶é¢ˆï¼Œä»è€Œæé«˜ GeoMX ç³»ç»Ÿçš„æ•ˆç‡ã€å¯æ‰©å±•æ€§å’Œæ•´ä½“æ€§èƒ½ã€‚"

#: ../../source/index.rst:49 cbcd1332aa1b49e3bb968f1d87c360c8
msgid "Synchronization Algorithms:"
msgstr "åŒæ­¥ä¼˜åŒ–ç®—æ³•ä¸€è§ˆï¼š"

#: ../../source/index.rst:50 a6f66aed28c743f1b10725c34c9ccd42
msgid ""
"GeoMX supports two fundamental synchronization algorithms: a fully-"
"synchronous algorithm and a mixed-synchronous algorithm."
msgstr "GeoMX æ”¯æŒä¸¤ç§åŸºç¡€çš„åŒæ­¥ç®—æ³•ï¼šå…¨åŒæ­¥ç®—æ³•å’Œæ··åˆåŒæ­¥ç®—æ³•ã€‚"

#: ../../source/index.rst:52 26f83b04d32b41cb99bb12e8d4de6a63
msgid ""
"**Fully-Synchronous Algorithm (FSA)**: In this synchronous algorithm, "
"training nodes synchronize their model data (can be parameters or "
"gradients) each round, and both parameter server systems within and "
"between data centers run in a synchronous parallel mode. This means all "
"training nodes are synchronized to ensure a consistent model. (Default)"
msgstr "**å…¨åŒæ­¥ç®—æ³• (FSA)**ï¼šåœ¨è¿™ç§åŒæ­¥ç®—æ³•ä¸­ï¼Œè®­ç»ƒèŠ‚ç‚¹åœ¨æ¯ä¸€è½®éƒ½åŒæ­¥å®ƒä»¬çš„æ¨¡å‹"
"æ•°æ®ï¼ˆæ¨¡å‹å‚æ•°æˆ–æ¢¯åº¦ï¼‰ï¼Œæ•°æ®ä¸­å¿ƒå†…éƒ¨å’Œæ•°æ®ä¸­å¿ƒä¹‹é—´çš„å‚æ•°æœåŠ¡å™¨ç³»ç»Ÿéƒ½ä»¥åŒæ­¥å¹¶è¡Œæ¨¡å¼"
"è¿è¡Œã€‚è¿™æ„å‘³ç€æ‰€æœ‰è®­ç»ƒèŠ‚ç‚¹çš„æ¨¡å‹æ•°æ®éƒ½å°†è¢«åŒæ­¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ä¸€è‡´æ€§ã€‚ï¼ˆé»˜è®¤è®¾ç½®ï¼‰"

#: ../../source/index.rst:55 1833689a600647f381b30ce54444e40c
msgid ""
"NOTE: FSA is highly effective in maintaining model accuracy and "
"consistency, but at the expense of training speed, as it necessitates "
"waiting for all computations and communications to complete at every "
"iteration."
msgstr "æ³¨æ„ï¼šå…¨åŒæ­¥ç®—æ³•åœ¨ç»´æŠ¤æ¨¡å‹ç²¾åº¦å’Œä¸€è‡´æ€§æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œä½†ä»£ä»·æ˜¯ç‰ºç‰²è®­ç»ƒé€Ÿåº¦ï¼Œ"
"å› ä¸ºå®ƒéœ€è¦åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½åŒæ­¥æ¨¡å‹æ•°æ®å¹¶ç­‰å¾…æ‰€æœ‰è®¡ç®—å’Œé€šä¿¡å®Œæˆã€‚"

#: ../../source/index.rst:57 e6df7504c73640368101a1a6fd5111c5
msgid ""
"**Mixed-Synchronous Algorithm (MixedSync)**: This algorithm is an "
"asynchronous version of FSA, where the difference is that the parameter "
"server system between data centers runs in an asynchronous parallel mode."
" This setup is particularly suitable for scenarios where intra-data "
"center training nodes display high homogeneity, yet there is significant "
"resource heterogeneity between different data centers. This asynchronous "
"method resolves the problem of straggling data centers, thereby "
"accelerating distributed training across WANs."
msgstr "**æ··åˆåŒæ­¥ç®—æ³• (MixedSync)**ï¼šè¯¥ç®—æ³•æ˜¯å…¨åŒæ­¥ç®—æ³•çš„å¼‚æ­¥ç‰ˆæœ¬ï¼ŒåŒºåˆ«åœ¨äºæ•°æ®ä¸­å¿ƒ"
"ä¹‹é—´çš„å‚æ•°æœåŠ¡å™¨ç³»ç»Ÿä»¥å¼‚æ­¥å¹¶è¡Œæ¨¡å¼è¿è¡Œã€‚è¿™ç§ç®—æ³•é€‚ç”¨äºæ•°æ®ä¸­å¿ƒå†…éƒ¨è®­ç»ƒèŠ‚ç‚¹æ˜¾ç¤ºå‡ºåŒè´¨æ€§ï¼Œ"
"ä½†ä¸åŒæ•°æ®ä¸­å¿ƒä¹‹é—´çš„èµ„æºå¼‚è´¨è¾ƒå¼ºçš„æƒ…å†µã€‚å¼‚æ­¥ç®—æ³•è§£å†³äº†ä½æ€§èƒ½æ•°æ®ä¸­å¿ƒæ‰é˜Ÿçš„é—®é¢˜ï¼Œèƒ½å¤ŸåŠ é€Ÿ"
"å¹¿åŸŸåˆ†å¸ƒå¼æœºå™¨å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ã€‚"

#: ../../source/index.rst:60 2999eee93840435cb9c660181392f4e8
msgid ""
"NOTE: To alleviate the issue of stale gradients in asynchronous parallel "
"operations, the global parameter server can be configured to use the "
"`DCASGD <http://proceedings.mlr.press/v70/zheng17b/zheng17b.pdf>`_ "
"optimizer. This adjustment aids in improving training convergence while "
"preserving model accuracy."
msgstr "æ³¨æ„ï¼šä¸ºäº†ç¼“è§£å¼‚æ­¥å¹¶è¡Œä¸­çš„å»¶è¿Ÿæ¢¯åº¦é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥é…ç½®å…¨å±€å‚æ•°æœåŠ¡å™¨ä½¿ç”¨"
" `DCASGD <http://proceedings.mlr.press/v70/zheng17b/zheng17b.pdf>`_ "
"ä¼˜åŒ–å™¨ã€‚è¿™ä¸ªä¼˜åŒ–å™¨å¯ä»¥è‡ªåŠ¨è¡¥å¿å»¶è¿Ÿæ¢¯åº¦ï¼Œæœ‰åŠ©äºæé«˜åˆ†å¸ƒå¼è®­ç»ƒçš„æ”¶æ•›æ€§ã€‚"

#: ../../source/index.rst:62 8f9eba7ff543484ebebf82ba129fe588
msgid ""
"Building upon the two aforementioned fundamental algorithms, GeoMX also "
"offers two advanced synchronization algorithms. These two advanced "
"algorithms are specifically designed to address the challenges of "
"bandwidth scarcity and resource heterogeneity in WANs."
msgstr "åŸºäºä¸Šè¿°ä¸¤ç§åŸºç¡€åŒæ­¥ç®—æ³•ï¼ŒGeoMX è¿˜æä¾›äº†ä¸¤ç§ä¼˜åŒ–åçš„åŒæ­¥ç®—æ³•ï¼Œä¸“é—¨è®¾è®¡ç”¨æ¥"
"è§£å†³å¹¿åŸŸç½‘ä¸­å¸¦å®½ç¨€ç¼ºå’Œèµ„æºå¼‚è´¨æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚"

#: ../../source/index.rst:64 8b9c1bfd8b5e444f86c7f6d36008ccb2
msgid ""
"**Hierarchical Frequency Aggregation (HFA)**: Inspired by `this paper "
"<https://ieeexplore.ieee.org/abstract/document/9148862>`_, our HFA "
"algorithm first performs :math:`K_1` steps of local updates at the "
"training nodes, followed by :math:`K_2` steps of synchronizations at the "
"local parameter server. Finally, a global synchronization is performed at"
" the global parameter server. This approach effectively reduces the "
"frequency of model synchronization across data centers, thereby boosting "
"distributed training."
msgstr "**åˆ†å±‚é¢‘ç‡èšåˆ (HFA)**ï¼šå—åˆ°è¿™ç¯‡æ–‡ç« "
" (`paper <https://ieeexplore.ieee.org/abstract/document/9148862>`_)"
"çš„å¯å‘ï¼Œæˆ‘ä»¬çš„ HFA ç®—æ³•é¦–å…ˆåœ¨è®­ç»ƒèŠ‚ç‚¹ä¸Šæ‰§è¡Œ :math:`K_1` æ­¥æœ¬åœ°æ›´æ–°ï¼Œç„¶ååœ¨åŸŸ"
"å†…å‚æ•°æœåŠ¡å™¨è¿›è¡Œ :math:`K_2` æ¬¡å±€åŸŸåŒæ­¥ï¼Œç„¶åæ‰åœ¨å…¨å±€å‚æ•°æœåŠ¡å™¨è¿›è¡Œä¸€æ¬¡å…¨å±€åŒæ­¥ã€‚"
"è¿™ç§æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†è·¨æ•°æ®ä¸­å¿ƒçš„æ¨¡å‹åŒæ­¥é¢‘ç‡ï¼Œä»è€Œèƒ½å¤Ÿå¤§å¹…æå‡åˆ†å¸ƒå¼è®­ç»ƒçš„æ•ˆç‡ã€‚"

#: ../../source/index.rst:66 60010757393c4b43bf5e91766a4faae3
msgid ""
"**ESync**: Applying asynchronous algorithms to strongly heterogeneous "
"clusters can lead to severe stale gradient issues. To address this, we "
"can adopt an optimized algorithm known as ESync. ESync is a synchronous "
"parallel algorithm designed to save stragglers under conditions of strong"
" resource heterogeneity. It introduces a state server to orchestrate the "
"local iteration steps of each training node, in order to balance their "
"reach-server time (including computational and transmission time). To be "
"integrated, refer to `this paper <https://drive.google.com/file/d"
"/1bvK0EeO5vjkXveU_ccBp4Uxl-qmbmcfn/view>`_ for more details and `this "
"repo <https://github.com/Lizonghang/ESync>`_ for individual use."
msgstr "**ESync**ï¼šå®è·µè¡¨æ˜ï¼Œå°†å¼‚æ­¥ç®—æ³•åº”ç”¨åˆ°å¼ºå¼‚è´¨èµ„æºçš„é›†ç¾¤ä¼šå¯¼è‡´ä¸¥é‡çš„å»¶è¿Ÿæ¢¯åº¦é—®é¢˜"
"å’Œæ”¶æ•›æ€§æŸä¼¤ã€‚äºæ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¸€ç§åä¸º ESync çš„ä¼˜åŒ–ç®—æ³•ã€‚ESync æ˜¯ä¸€ç§åŒæ­¥å¹¶è¡Œç®—æ³•ï¼Œ"
"è®¾è®¡ç”¨æ¥åœ¨å¼ºèµ„æºå¼‚è´¨çš„æ¡ä»¶ä¸‹è§£å†³åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„æ‰é˜ŸèŠ‚ç‚¹é—®é¢˜ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªçŠ¶æ€æŸ¥è¯¢æœåŠ¡å™¨"
"æ¥åè°ƒæ¯ä¸ªè®­ç»ƒèŠ‚ç‚¹çš„æœ¬åœ°è¿­ä»£æ¬¡æ•°ï¼Œä»¥å¹³è¡¡ä»–ä»¬çš„æ¨¡å‹æ•°æ®åˆ°è¾¾å‚æ•°æœåŠ¡å™¨çš„æ—¶é—´ã€‚å¦‚æœæ‚¨æƒ³äº†è§£"
"æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒè¿™ç¯‡æ–‡ç«  (`Paper <https://drive.google.com/file/d/1bvK0EeO5vjkXveU_ccBp4Uxl-qmbmcfn/view>`_)ï¼Œ"
"è¯¥åŠŸèƒ½æ­£åœ¨é›†æˆä¸­ï¼Œå¦‚éœ€æå‰ä½“éªŒè¯·å°è¯•è¿™ä¸ªä»£ç åº“ (`Repo <https://github.com/Lizonghang/ESync>`_)ã€‚"

#: ../../source/index.rst:69 f7be0b2e43ef4cb784b8ad0ef1b8d1f6
msgid "For more details on the GeoMX system, please refer to our book:"
msgstr "å¦‚éœ€äº†è§£æ›´å¤šå…³äº GeoMX ç³»ç»Ÿçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„ä¹¦ç±ã€‚"

#: ../../source/index.rst:71 a449ac49156347e69b3655eef7e11674
msgid ""
"**è™çº¢èŠ³, æå®—èˆª, å­™ç½¡, ç½—é¾™. ã€Šè·¨æ•°æ®ä¸­å¿ƒæœºå™¨å­¦ä¹ ï¼šèµ‹èƒ½å¤šäº‘æ™ºèƒ½æ•°ç®—èåˆã€‹. ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾, äººå·¥æ™ºèƒ½å‰æ²¿ç†è®ºä¸æŠ€æœ¯åº”ç”¨ä¸›ä¹¦, "
"(2023).**"
msgstr ""

#: ../../source/index.rst 9946a8827bf246a78830ba608c9edb42
msgid "Cover for GeoMX Book"
msgstr ""

