# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Zonghang Li
# This file is distributed under the same license as the GeoMX package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.

msgid ""
msgstr ""
"Project-Id-Version: GeoMX 1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-02 09:50+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: Zonghang Li <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <lizhuestc@gmail.com>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/index.rst:11
msgid "GeoMX Tutorials"
msgstr "GeoMX æ•™ç¨‹"

#: ../../source/index.rst:7 ee1a1e4273a34dcea8ee2b229efe3e28
msgid "Welcome to GeoMX Docs! ğŸ˜"
msgstr "æ¬¢è¿æ¥åˆ° GeoMX çš„ä¸­æ–‡æ–‡æ¡£ï¼ğŸ˜"

#: ../../source/index.rst:26 32bf00cb9bc74913bbf629a4d0ec3647
msgid ""
"**GeoMX** is a fast and unified distributed system for training ML "
"algorithms over geographical data centers. Built upon the `MXNET "
"<https://github.com/apache/mxnet>`_ framework, GeoMX integrates several "
"sophisticated optimization techniques to enhance its training efficiency."
" These strategic enhancements result in a significant performance boost "
"compared to the original MXNET system, offering 20x acceleration under "
"identical network bandwidth conditions. This superior efficiency propels "
"GeoMX to the forefront of training systems used for geographically "
"dispersed data centers, showcasing satisfying performance and "
"effectiveness."
msgstr "GeoMX æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼Œå®ƒç”¨äºåœ¨åœ°ç†åˆ†æ•£çš„å¤šä¸ªæ•°æ®ä¸­å¿ƒä¹‹é—´"
"è¿›è¡ŒååŒçš„æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒã€‚GeoMX æ˜¯åŸºäº `MXNET <https://github.com/apache/mxnet>`_ "
"ç³»ç»Ÿå»ºç«‹çš„ï¼Œå¹¶ä¸”èåˆäº†ä¸€ç³»åˆ—å¤æ‚çš„ä¼˜åŒ–æŠ€æœ¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡è¿™äº›æ”¹è¿›ï¼Œç›¸æ¯”äºåŸå§‹çš„ MXNET "
"ç³»ç»Ÿï¼Œåœ¨ç›¸åŒç½‘ç»œå¸¦å®½çš„æ¡ä»¶ä¸‹ï¼ŒGeoMX çš„è®­ç»ƒé€Ÿåº¦æå‡äº† 20 å€ã€‚è¿™ç§æ˜¾è‘—çš„æ•ˆç‡æå‡ä½¿å¾— GeoMX "
"é€‚ç”¨äºåœ°ç†åˆ†æ•£çš„æ•°æ®ä¸­å¿ƒåœºæ™¯ï¼Œåœ¨è·¨å¹¿åŸŸçš„åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ åº”ç”¨ä¸­è¡¨ç°å‡ºäº†ä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚"

#: ../../source/index.rst:28 284dc6e7dd40416b8252f441ed8a2f57
msgid ""
"GeoMX employs the `Hierarchical Parameter Server (HiPS) "
"<https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-"
"cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_"
" framework as its fundamental training architecture, designed to "
"segregate the network environment within and beyond the data center. This"
" unique architecture includes an intra-domain parameter server system "
"within each data center while concurrently establishing an inter-"
"parameter server connection between different data centers. In this "
"configuration, model data traffic undergoes two stages of aggregation: an"
" initial local aggregation within each data center, followed by a global "
"aggregation at the central data center. This approach effectively "
"minimizes cross-WAN traffic and, consequently, reduces communication "
"overhead."
msgstr "GeoMX é‡‡ç”¨ä¸€ç§åä¸ºåˆ†å±‚å‚æ•°æœåŠ¡å™¨ (`HiPS <https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_) "
"çš„æ–°å‹æ¶æ„ä½œä¸ºå®ƒçš„åŸºç¡€è®­ç»ƒæ¶æ„ã€‚HiPS æ¶æ„åˆ†éš”äº†æ•°æ®ä¸­å¿ƒå†…å¤–éƒ¨ç½‘ç»œç¯å¢ƒï¼Œå®ƒåœ¨æ¯ä¸ªæ•°æ®"
"ä¸­å¿ƒå†…éƒ¨å»ºç«‹åŸŸå†…å‚æ•°æœåŠ¡å™¨ç³»ç»Ÿï¼ŒåŒæ—¶åœ¨ä¸åŒçš„æ•°æ®ä¸­å¿ƒä¹‹é—´ä¹Ÿå»ºç«‹èµ·äº†å…¨å±€å‚æ•°æœåŠ¡å™¨ç³»ç»Ÿã€‚"
"åœ¨è¿™ç§è®¾ç½®ä¸‹ï¼Œæ¨¡å‹æ•°æ®çš„ä¼ è¾“ä¼šç»å†ä¸¤ä¸ªé˜¶æ®µçš„èšåˆï¼šé¦–å…ˆï¼Œåœ¨å„è‡ªçš„æ•°æ®ä¸­å¿ƒå†…éƒ¨å±€éƒ¨èšåˆï¼Œ"
"ç„¶åï¼Œåœ¨å…¨å±€æ•°æ®ä¸­å¿ƒè¿›è¡Œå…¨å±€èšåˆã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆå‡å°äº†è·¨å¹¿åŸŸç½‘çš„é€šä¿¡æµé‡ï¼Œä»è€Œæ˜¾è‘—é™ä½é€šä¿¡å¼€é”€ã€‚"

#: ../../source/index.rst:30 0b759d2159d244808f1b652d2b99f79c
msgid ""
"But it's far from enough, given the often limited and varied network "
"conditions in WANs, distributed training across data centers can "
"potentially create communication bottlenecks. To mitigate these issues, "
"GeoMX employs a variety of optimization techniques. These include "
"gradient sparsification, low-precision quantization (e.g., fp16), mixed-"
"precision quantization, advanced transmission protocols, synchronization "
"algorithms, flow scheduling, and priority scheduling, among others (e.g.,"
" overlay scheduling, currently in development). These techniques "
"comprehensively tackle communication issues, further enhancing the "
"efficiency and robustness of distributed machine learning training in "
"GeoMX."
msgstr "ä½†æ˜¯ï¼Œä»…æœ‰è¿™äº›è¿˜è¿œè¿œä¸å¤Ÿã€‚è€ƒè™‘åˆ°å¹¿åŸŸç½‘ä¸­çš„ç½‘ç»œèµ„æºå¸¸å¸¸æ˜¯å—é™çš„ï¼Œå¹¶ä¸”éšæ—¶é—´"
"åŠ¨æ€æ—¶å˜ï¼Œè·¨æ•°æ®ä¸­å¿ƒçš„åˆ†å¸ƒå¼è®­ç»ƒä»ç„¶é¢ä¸´è¯¸å¤šé€šä¿¡ç“¶é¢ˆã€‚ä¸ºäº†ç¼“è§£è¿™äº›ç“¶é¢ˆï¼ŒGeoMX é‡‡ç”¨"
"äº†å¤šç§ä¼˜åŒ–æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯åŒ…æ‹¬æ¢¯åº¦ç¨€ç–åŒ–ã€ä½ç²¾åº¦é‡åŒ–ï¼ˆä¾‹å¦‚ FP16ï¼‰ã€æ··åˆç²¾åº¦é‡åŒ–ã€æ›´å…ˆ"
"è¿›çš„ä¼ è¾“åè®®ã€åŒæ­¥ç®—æ³•ã€æµé‡è°ƒåº¦ã€ä¼˜å…ˆçº§è°ƒåº¦ï¼Œä»¥åŠå…¶ä»–ä¸€äº›æ­£åœ¨å¼€å‘æˆ–é›†æˆçš„æŠ€æœ¯ï¼Œä¾‹å¦‚"
"é€šä¿¡è¦†ç›–è°ƒåº¦ç­‰ï¼‰ã€‚è¿™äº›æŠ€æœ¯å…¨é¢åœ°è§£å†³äº†è·¨å¹¿åŸŸåˆ†å¸ƒå¼ç³»ç»Ÿçš„é€šä¿¡é—®é¢˜ï¼Œè¿›ä¸€æ­¥æé«˜äº† GeoMX "
"ç³»ç»Ÿè®­ç»ƒçš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚"

#: ../../source/index.rst:33 7200adb078194f048588fd2ebe217a8a
msgid "Optimization Techniques:"
msgstr "ä¼˜åŒ–æŠ€æœ¯ä¸€è§ˆï¼š"

#: ../../source/index.rst:34 7b928f7b0c9641b792fed12d34744316
msgid ""
"**Bidirectional Gradient Sparsification (Bi-Sparse)**: Traditional "
"approaches such as `Deep Gradient Compression "
"<https://arxiv.org/pdf/1712.01887.pdf>`_ sparsify the pushed gradient "
"tensors. For further compression, we also sparsify the pulled "
"(aggregated) gradient tensors rather than pulling full parameters. This "
"technique is enabled between the global parameter server and the intra-"
"domain parameter servers of different data centers. Refer to `this paper "
"<https://www.zte.com.cn/content/dam/zte-site/res-www-zte-com-"
"cn/mediares/magazine/publication/com_cn/article/202005/cn202005004.pdf>`_"
" for more details."
msgstr "**åŒå‘æ¢¯åº¦ç¨€ç–åŒ– (Bi-Sparse)**ï¼š"

#: ../../source/index.rst:36 d827a6445ea5450b86c296e77ccafe9c
msgid ""
"**Low-Precision Quantization (FP16)**: GeoMX also supports quantifying "
"model data at lower precision for transmission, such as in FP16 format. "
"In this scheme, GeoMX computes the model using FP32, but during "
"transmission, it converts the model data tensor into FP16. Once the "
"pulling data is received, GeoMX reverts it back into FP32 and continues "
"model computing. This effectively halves the data traffic volume over "
"both LANs and WANs."
msgstr ""

#: ../../source/index.rst:38 ae1dc692222d417e8a727bbc34c57c70
msgid ""
"**Mixed-Precision Quantization (MPQ)**: The technology of MPQ leverages "
"both Bi-Sparse and FP16. In this scheme, tiny tensors are quantified into"
" FP16 format for transmission, while large tensors persist in the FP32 "
"format. Moreover, these large sensors will undergo a sparsification "
"process before transmission. This precaution is taken to minimize the "
"loss of crucial information and avoid significant degradation to model "
"performance."
msgstr ""

#: ../../source/index.rst:40 91dc5c1f87fc493683fd0e6e56b2516a
msgid ""
"**Differential Gradient Transmission (DGT)**: This advanced transmission "
"protocol is optimized for distributed machine learning tasks. Leveraging "
"the tolerance of gradient descent algorithms towards partial parameter "
"loss, this protocol transfers gradients across multiple channels, each "
"with distinct levels of reliability and priority, contingent on their "
"respective contributions to model convergence. Through these prioritized "
"channels, critical gradients receive precedence in transmission, while "
"other non-important gradients are transmitted with lower priority and "
"reliability. This helps to reduce tail latency and thus reduce the end-"
"to-end transmission delay of parameter synchronization. Refer to `this "
"paper "
"<https://drive.google.com/file/d/1IbmpFybX_qXZM2g_8BrcD9IF080qci94/view>`_"
" for more details and `this repo <https://github.com/zhouhuaman/dgt>`_ "
"for individual use."
msgstr ""

#: ../../source/index.rst:42 363b27f16b2a4eeaa84a3813de65f49b
msgid ""
"**TSEngine**: To solve the communication in-cast issue typically "
"associated with centralized parameter servers, GeoMX incorporates "
"TSEngine, an adaptive communication scheduler designed for efficient "
"communication overlay in WANs. TSEngine dynamically optimizes the "
"topology overlay and communication logic among the training nodes in "
"response to real-time network conditions. This adaptive scheduler shows "
"significant advantages over existing communication patterns in terms of "
"system efficiency, communication, as well as scalability. Refer to `this "
"paper <https://drive.google.com/file/d/1ELfApVoCA8WCdOe3iBe-"
"VreLJCSD7r8r/view>`_ for more details and `this repo "
"<https://github.com/zhouhuaman/TSEngine>`_ for individual use."
msgstr ""

#: ../../source/index.rst:44 fd7375c58c1e4c568ce2cbd1f39f5638
msgid ""
"**Priority-based Parameter Propagation (P3)**: In conventional "
"implementations, the gradient synchronization at round :math:`r` does not"
" overlap with the forward propagation at round :math:`r+1`, because the "
"forward propagation relies on the completion of gradient synchronization."
" To improve system efficiency, GeoMX integrates the P3 scheduler, which "
"prioritizes the transmission of shallow-layer gradients. This setup "
"enables overlapping between forward propagation and gradient "
"synchronization, allowing earlier execution of forward propagation for "
"the next round, thereby accelerating distributed training. See `this "
"paper <https://arxiv.org/pdf/1905.03960.pdf>`_ for more details and `this"
" repo <https://github.com/anandj91/p3>`_ for individual use."
msgstr ""

#: ../../source/index.rst:46 a4a2109b167c4a93b074adfc7c6fa5b1
msgid ""
"**Multi-Server Load Balancing (MultiGPS)**: GeoMX supports a balanced "
"distribution of workload, including traffic, storage, and computation, "
"across multiple global parameter servers. By preventing any single server"
" from becoming a bottleneck, MultiGPS significantly enhances efficiency, "
"scalability, and overall performance of our GeoMX system."
msgstr ""

#: ../../source/index.rst:49 cbcd1332aa1b49e3bb968f1d87c360c8
msgid "Synchronization Algorithms:"
msgstr ""

#: ../../source/index.rst:50 a6f66aed28c743f1b10725c34c9ccd42
msgid ""
"GeoMX supports two fundamental synchronization algorithms: a fully-"
"synchronous algorithm and a mixed-synchronous algorithm."
msgstr ""

#: ../../source/index.rst:52 26f83b04d32b41cb99bb12e8d4de6a63
msgid ""
"**Fully-Synchronous Algorithm (FSA)**: In this synchronous algorithm, "
"training nodes synchronize their model data (can be parameters or "
"gradients) each round, and both parameter server systems within and "
"between data centers run in a synchronous parallel mode. This means all "
"training nodes are synchronized to ensure a consistent model. (Default)"
msgstr ""

#: ../../source/index.rst:55 1833689a600647f381b30ce54444e40c
msgid ""
"NOTE: FSA is highly effective in maintaining model accuracy and "
"consistency, but at the expense of training speed, as it necessitates "
"waiting for all computations and communications to complete at every "
"iteration."
msgstr ""

#: ../../source/index.rst:57 e6df7504c73640368101a1a6fd5111c5
msgid ""
"**Mixed-Synchronous Algorithm (MixedSync)**: This algorithm is an "
"asynchronous version of FSA, where the difference is that the parameter "
"server system between data centers runs in an asynchronous parallel mode."
" This setup is particularly suitable for scenarios where intra-data "
"center training nodes display high homogeneity, yet there is significant "
"resource heterogeneity between different data centers. This asynchronous "
"method resolves the problem of straggling data centers, thereby "
"accelerating distributed training across WANs."
msgstr ""

#: ../../source/index.rst:60 2999eee93840435cb9c660181392f4e8
msgid ""
"NOTE: To alleviate the issue of stale gradients in asynchronous parallel "
"operations, the global parameter server can be configured to use the "
"`DCASGD <http://proceedings.mlr.press/v70/zheng17b/zheng17b.pdf>`_ "
"optimizer. This adjustment aids in improving training convergence while "
"preserving model accuracy."
msgstr ""

#: ../../source/index.rst:62 8f9eba7ff543484ebebf82ba129fe588
msgid ""
"Building upon the two aforementioned fundamental algorithms, GeoMX also "
"offers two advanced synchronization algorithms. These two advanced "
"algorithms are specifically designed to address the challenges of "
"bandwidth scarcity and resource heterogeneity in WANs."
msgstr ""

#: ../../source/index.rst:64 8b9c1bfd8b5e444f86c7f6d36008ccb2
msgid ""
"**Hierarchical Frequency Aggregation (HFA)**: Inspired by `this paper "
"<https://ieeexplore.ieee.org/abstract/document/9148862>`_, our HFA "
"algorithm first performs :math:`K_1` steps of local updates at the "
"training nodes, followed by :math:`K_2` steps of synchronizations at the "
"local parameter server. Finally, a global synchronization is performed at"
" the global parameter server. This approach effectively reduces the "
"frequency of model synchronization across data centers, thereby boosting "
"distributed training."
msgstr ""

#: ../../source/index.rst:66 60010757393c4b43bf5e91766a4faae3
msgid ""
"**ESync**: Applying asynchronous algorithms to strongly heterogeneous "
"clusters can lead to severe stale gradient issues. To address this, we "
"can adopt an optimized algorithm known as ESync. ESync is a synchronous "
"parallel algorithm designed to save stragglers under conditions of strong"
" resource heterogeneity. It introduces a state server to orchestrate the "
"local iteration steps of each training node, in order to balance their "
"reach-server time (including computational and transmission time). To be "
"integrated, refer to `this paper <https://drive.google.com/file/d"
"/1bvK0EeO5vjkXveU_ccBp4Uxl-qmbmcfn/view>`_ for more details and `this "
"repo <https://github.com/Lizonghang/ESync>`_ for individual use."
msgstr ""

#: ../../source/index.rst:69 f7be0b2e43ef4cb784b8ad0ef1b8d1f6
msgid "For more details on the GeoMX system, please refer to our book:"
msgstr ""

#: ../../source/index.rst:71 a449ac49156347e69b3655eef7e11674
msgid ""
"**è™çº¢èŠ³, æå®—èˆª, å­™ç½¡, ç½—é¾™. ã€Šè·¨æ•°æ®ä¸­å¿ƒæœºå™¨å­¦ä¹ ï¼šèµ‹èƒ½å¤šäº‘æ™ºèƒ½æ•°ç®—èåˆã€‹. ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾, äººå·¥æ™ºèƒ½å‰æ²¿ç†è®ºä¸æŠ€æœ¯åº”ç”¨ä¸›ä¹¦, "
"(2023).**"
msgstr ""

#: ../../source/index.rst 9946a8827bf246a78830ba608c9edb42
msgid "Cover for GeoMX Book"
msgstr ""

